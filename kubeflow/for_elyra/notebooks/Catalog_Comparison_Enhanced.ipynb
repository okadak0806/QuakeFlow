{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "catalog-comparison-title",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üìä Enhanced Catalog Comparison\n",
    "\n",
    "This notebook performs comprehensive comparison between reference earthquake catalogs and detected events from the QuakeFlow pipeline.\n",
    "\n",
    "## Features\n",
    "- **Reference Catalog Download**: Automatic download from USGS, JMA, NCEDC, etc.\n",
    "- **Event Matching**: Sophisticated spatiotemporal matching algorithms\n",
    "- **Performance Metrics**: Precision, recall, F1-score, and detection rates\n",
    "- **Comprehensive Visualization**: Maps, residual plots, and performance charts\n",
    "- **Detailed Reporting**: HTML reports with interpretation and recommendations\n",
    "\n",
    "## Required Inputs\n",
    "- Configuration file (config/config.json)\n",
    "- Detected events catalog (gamma/gamma_catalog.csv)\n",
    "- Station information (stations/stations.json)\n",
    "\n",
    "## Outputs\n",
    "- Reference catalog (comparison/reference_catalog.csv)\n",
    "- Event matches (comparison/event_matches.csv)\n",
    "- Performance metrics (comparison/performance_metrics.json)\n",
    "- Visualization plots (comparison/figures/)\n",
    "- HTML comparison report (comparison/comparison_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enhanced catalog comparison setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add common utilities to path\n",
    "sys.path.append('../../examples')\n",
    "\n",
    "# Import QuakeFlow enhanced utilities\n",
    "from common import (\n",
    "    notebook_setup, \n",
    "    notebook_finalize, \n",
    "    CatalogComparison,\n",
    "    ElyraUtils\n",
    ")\n",
    "\n",
    "print(\"üìä Enhanced Catalog Comparison Notebook Initialized\")\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "notebook-setup",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enhanced notebook setup with automatic configuration\n",
    "try:\n",
    "    config, workflow, parallel_config = notebook_setup()\n",
    "    \n",
    "    print(f\"\\nüåç Region Configuration Loaded: {config.region.upper()}\")\n",
    "    print(f\"Geographic Bounds: {config.get_geographic_bounds()}\")\n",
    "    print(f\"Parallel Configuration: {parallel_config}\")\n",
    "    \n",
    "    # Environment detection\n",
    "    is_elyra = ElyraUtils.is_elyra_environment()\n",
    "    is_kubeflow = ElyraUtils.is_kubeflow_environment()\n",
    "    \n",
    "    print(f\"\\nüîß Environment Detection:\")\n",
    "    print(f\"  Elyra Environment: {is_elyra}\")\n",
    "    print(f\"  Kubeflow Environment: {is_kubeflow}\")\n",
    "    \n",
    "    # Initialize catalog comparison\n",
    "    comparison = CatalogComparison(config)\n",
    "    print(f\"\\nüìä Catalog Comparison Initialized\")\n",
    "    print(f\"  Matching Criteria: {comparison.matching_criteria}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environment-configuration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure comparison parameters from environment variables\n",
    "REFERENCE_SOURCE = os.environ.get('REFERENCE_SOURCE', 'usgs').lower()\n",
    "MIN_MAGNITUDE = float(os.environ.get('MIN_MAGNITUDE', '2.0'))\n",
    "MAX_MAGNITUDE = float(os.environ.get('MAX_MAGNITUDE', '9.0'))\n",
    "START_TIME = os.environ.get('START_TIME', None)\n",
    "END_TIME = os.environ.get('END_TIME', None)\n",
    "\n",
    "# Override matching criteria if specified\n",
    "TIME_WINDOW = float(os.environ.get('TIME_WINDOW', '30.0'))\n",
    "DISTANCE_THRESHOLD = float(os.environ.get('DISTANCE_THRESHOLD', '10.0'))\n",
    "MAG_DIFF_THRESHOLD = float(os.environ.get('MAG_DIFF_THRESHOLD', '1.0'))\n",
    "\n",
    "# Update comparison criteria\n",
    "comparison.matching_criteria.update({\n",
    "    'time_window': TIME_WINDOW,\n",
    "    'distance_threshold': DISTANCE_THRESHOLD,\n",
    "    'magnitude_diff_threshold': MAG_DIFF_THRESHOLD\n",
    "})\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Comparison Configuration:\")\n",
    "print(f\"  Reference Source: {REFERENCE_SOURCE.upper()}\")\n",
    "print(f\"  Magnitude Range: {MIN_MAGNITUDE} - {MAX_MAGNITUDE}\")\n",
    "print(f\"  Time Range: {START_TIME or 'Auto'} to {END_TIME or 'Auto'}\")\n",
    "print(f\"  Updated Matching Criteria: {comparison.matching_criteria}\")\n",
    "\n",
    "# Define output directories\n",
    "output_base = os.environ.get('OUTPUT_DIR', 'comparison')\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "os.makedirs(f\"{output_base}/figures\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Output Directory: {output_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-reference-catalog",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download reference catalog from specified source\n",
    "print(f\"\\nüåê Downloading Reference Catalog from {REFERENCE_SOURCE.upper()}...\")\n",
    "\n",
    "try:\n",
    "    # Determine time range if not specified\n",
    "    if START_TIME is None or END_TIME is None:\n",
    "        # Try to extract from detected catalog if available\n",
    "        detected_catalog_path = 'gamma/gamma_catalog.csv'\n",
    "        if os.path.exists(detected_catalog_path):\n",
    "            temp_detected = pd.read_csv(detected_catalog_path)\n",
    "            if 'time' in temp_detected.columns:\n",
    "                temp_times = pd.to_datetime(temp_detected['time'])\n",
    "                auto_start = temp_times.min() - timedelta(hours=1)\n",
    "                auto_end = temp_times.max() + timedelta(hours=1)\n",
    "                START_TIME = START_TIME or auto_start.isoformat()\n",
    "                END_TIME = END_TIME or auto_end.isoformat()\n",
    "                print(f\"  Auto-detected time range from detected catalog:\")\n",
    "                print(f\"    Start: {START_TIME}\")\n",
    "                print(f\"    End: {END_TIME}\")\n",
    "    \n",
    "    # Download reference catalog\n",
    "    reference_catalog = comparison.download_reference_catalog(\n",
    "        source=REFERENCE_SOURCE,\n",
    "        start_time=START_TIME,\n",
    "        end_time=END_TIME,\n",
    "        min_magnitude=MIN_MAGNITUDE,\n",
    "        max_magnitude=MAX_MAGNITUDE\n",
    "    )\n",
    "    \n",
    "    if len(reference_catalog) == 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: No reference events downloaded from {REFERENCE_SOURCE.upper()}\")\n",
    "        print(\"   This may be due to:\")\n",
    "        print(\"   - No events in the specified time/magnitude/location range\")\n",
    "        print(\"   - Network connectivity issues\")\n",
    "        print(\"   - Service unavailability\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Successfully downloaded {len(reference_catalog)} reference events\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(f\"\\nüìà Reference Catalog Summary:\")\n",
    "        print(f\"  Time Range: {reference_catalog['time'].min()} to {reference_catalog['time'].max()}\")\n",
    "        if 'magnitude' in reference_catalog.columns and not reference_catalog['magnitude'].isna().all():\n",
    "            mag_stats = reference_catalog['magnitude'].describe()\n",
    "            print(f\"  Magnitude Range: {mag_stats['min']:.1f} - {mag_stats['max']:.1f} (mean: {mag_stats['mean']:.1f})\")\n",
    "        print(f\"  Depth Range: {reference_catalog['depth_km'].min():.1f} - {reference_catalog['depth_km'].max():.1f} km\")\n",
    "        \n",
    "        # Save reference catalog\n",
    "        reference_catalog_path = f\"{output_base}/reference_catalog.csv\"\n",
    "        reference_catalog.to_csv(reference_catalog_path, index=False)\n",
    "        print(f\"\\nüíæ Reference catalog saved: {reference_catalog_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading reference catalog: {e}\")\n",
    "    print(\"   Creating empty reference catalog for demonstration\")\n",
    "    reference_catalog = pd.DataFrame(columns=[\n",
    "        'event_id', 'time', 'latitude', 'longitude', 'depth_km', \n",
    "        'magnitude', 'magnitude_type', 'source'\n",
    "    ])\n",
    "    \n",
    "print(f\"\\nüìä Reference catalog contains {len(reference_catalog)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-detected-catalog",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load detected events catalog from GaMMA output\n",
    "print(f\"\\nüìÇ Loading Detected Events Catalog...\")\n",
    "\n",
    "try:\n",
    "    # Possible detected catalog file paths\n",
    "    possible_paths = [\n",
    "        'gamma/gamma_catalog.csv',\n",
    "        'gamma/gamma_catalog_filtered.csv',\n",
    "        '../gamma/gamma_catalog.csv',\n",
    "        'events/events_gamma.csv'\n",
    "    ]\n",
    "    \n",
    "    detected_catalog = None\n",
    "    used_path = None\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            detected_catalog = comparison.load_detected_catalog(path)\n",
    "            used_path = path\n",
    "            break\n",
    "    \n",
    "    if detected_catalog is None or len(detected_catalog) == 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: No detected events found in standard locations\")\n",
    "        print(f\"   Searched paths: {possible_paths}\")\n",
    "        print(f\"   Creating mock detected catalog for demonstration\")\n",
    "        \n",
    "        # Create mock detected catalog for demonstration\n",
    "        bounds = config.get_geographic_bounds()\n",
    "        n_mock = 10\n",
    "        \n",
    "        mock_data = {\n",
    "            'event_id': [f'mock_{i:03d}' for i in range(n_mock)],\n",
    "            'time': pd.date_range(START_TIME or '2024-01-01', periods=n_mock, freq='H'),\n",
    "            'latitude': np.random.uniform(bounds['minlatitude'], bounds['maxlatitude'], n_mock),\n",
    "            'longitude': np.random.uniform(bounds['minlongitude'], bounds['maxlongitude'], n_mock),\n",
    "            'depth_km': np.random.uniform(0, 30, n_mock),\n",
    "            'magnitude': np.random.uniform(2.0, 5.0, n_mock),\n",
    "            'source': ['DETECTED'] * n_mock\n",
    "        }\n",
    "        \n",
    "        detected_catalog = pd.DataFrame(mock_data)\n",
    "        used_path = 'mock_catalog'\n",
    "        \n",
    "        print(f\"   Created {len(detected_catalog)} mock detected events\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Successfully loaded {len(detected_catalog)} detected events from {used_path}\")\n",
    "    \n",
    "    # Display detected catalog summary\n",
    "    print(f\"\\nüìà Detected Catalog Summary:\")\n",
    "    print(f\"  Source File: {used_path}\")\n",
    "    print(f\"  Event Count: {len(detected_catalog)}\")\n",
    "    \n",
    "    if len(detected_catalog) > 0:\n",
    "        print(f\"  Time Range: {detected_catalog['time'].min()} to {detected_catalog['time'].max()}\")\n",
    "        if 'magnitude' in detected_catalog.columns and not detected_catalog['magnitude'].isna().all():\n",
    "            mag_stats = detected_catalog['magnitude'].describe()\n",
    "            print(f\"  Magnitude Range: {mag_stats['min']:.1f} - {mag_stats['max']:.1f} (mean: {mag_stats['mean']:.1f})\")\n",
    "        print(f\"  Depth Range: {detected_catalog['depth_km'].min():.1f} - {detected_catalog['depth_km'].max():.1f} km\")\n",
    "        \n",
    "        # Save standardized detected catalog\n",
    "        detected_catalog_path = f\"{output_base}/detected_catalog.csv\"\n",
    "        detected_catalog.to_csv(detected_catalog_path, index=False)\n",
    "        print(f\"\\nüíæ Detected catalog saved: {detected_catalog_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading detected catalog: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perform-catalog-comparison",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform comprehensive catalog comparison\n",
    "print(f\"\\nüîç Performing Catalog Comparison...\")\n",
    "\n",
    "try:\n",
    "    if len(reference_catalog) == 0 and len(detected_catalog) == 0:\n",
    "        print(\"‚ö†Ô∏è Warning: Both reference and detected catalogs are empty\")\n",
    "        print(\"   Cannot perform meaningful comparison\")\n",
    "        \n",
    "        # Create minimal results for pipeline completion\n",
    "        comparison_results = {\n",
    "            'summary': {\n",
    "                'total_reference': 0,\n",
    "                'total_detected': 0,\n",
    "                'matched_pairs': 0,\n",
    "                'f1_score': 0.0,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0\n",
    "            },\n",
    "            'output_files': {\n",
    "                'metrics': f\"{output_base}/performance_metrics.json\",\n",
    "                'report': f\"{output_base}/comparison_report.html\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save minimal metrics\n",
    "        minimal_metrics = {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'detection_rate': 0.0,\n",
    "            'false_alarm_rate': 0.0,\n",
    "            'note': 'No data available for comparison'\n",
    "        }\n",
    "        \n",
    "        with open(f\"{output_base}/performance_metrics.json\", 'w') as f:\n",
    "            json.dump(minimal_metrics, f, indent=2)\n",
    "            \n",
    "        print(f\"   Saved minimal metrics to handle empty catalogs\")\n",
    "        \n",
    "    else:\n",
    "        # Run full comparison workflow\n",
    "        comparison_results = comparison.run_full_comparison(\n",
    "            reference_catalog=reference_catalog,\n",
    "            detected_catalog=detected_catalog,\n",
    "            reference_source=REFERENCE_SOURCE,\n",
    "            output_dir=output_base\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Catalog Comparison Completed Successfully!\")\n",
    "        print(f\"\\nüìä Performance Summary:\")\n",
    "        print(f\"  F1 Score: {comparison_results['summary']['f1_score']:.3f}\")\n",
    "        print(f\"  Precision: {comparison_results['summary']['precision']:.3f}\")\n",
    "        print(f\"  Recall: {comparison_results['summary']['recall']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüìà Event Statistics:\")\n",
    "        print(f\"  Reference Events: {comparison_results['summary']['total_reference_events']}\")\n",
    "        print(f\"  Detected Events: {comparison_results['summary']['total_detected_events']}\")\n",
    "        print(f\"  Matched Events: {comparison_results['summary']['matched_events']}\")\n",
    "        \n",
    "        # Display output files\n",
    "        print(f\"\\nüìÅ Generated Output Files:\")\n",
    "        for file_type, file_path in comparison_results['output_files'].items():\n",
    "            if file_path and os.path.exists(file_path):\n",
    "                print(f\"  {file_type}: {file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during catalog comparison: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-kubeflow-metadata",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Kubeflow UI metadata and finalize notebook\n",
    "print(f\"\\nüéØ Finalizing Catalog Comparison Results...\")\n",
    "\n",
    "try:\n",
    "    # Prepare results for finalization\n",
    "    results = {\n",
    "        'comparison_completed': True,\n",
    "        'reference_source': REFERENCE_SOURCE,\n",
    "        'reference_events_count': len(reference_catalog),\n",
    "        'detected_events_count': len(detected_catalog),\n",
    "        'matching_criteria': comparison.matching_criteria\n",
    "    }\n",
    "    \n",
    "    # Add performance metrics if comparison was performed\n",
    "    if 'summary' in comparison_results:\n",
    "        results.update({\n",
    "            'matched_events': comparison_results['summary'].get('matched_events', 0),\n",
    "            'f1_score': comparison_results['summary'].get('f1_score', 0.0),\n",
    "            'precision': comparison_results['summary'].get('precision', 0.0),\n",
    "            'recall': comparison_results['summary'].get('recall', 0.0)\n",
    "        })\n",
    "    \n",
    "    # Define output artifacts\n",
    "    artifacts = [\n",
    "        f\"{output_base}/reference_catalog.csv\",\n",
    "        f\"{output_base}/detected_catalog.csv\", \n",
    "        f\"{output_base}/performance_metrics.json\",\n",
    "        f\"{output_base}/comparison_report.html\"\n",
    "    ]\n",
    "    \n",
    "    # Add optional artifacts if they exist\n",
    "    optional_artifacts = [\n",
    "        f\"{output_base}/event_matches.csv\",\n",
    "        f\"{output_base}/unmatched_reference.csv\",\n",
    "        f\"{output_base}/unmatched_detected.csv\",\n",
    "        f\"{output_base}/figures/event_locations_comparison.png\",\n",
    "        f\"{output_base}/figures/performance_metrics.png\",\n",
    "        f\"{output_base}/figures/parameter_residuals.png\",\n",
    "        f\"{output_base}/figures/magnitude_performance.png\"\n",
    "    ]\n",
    "    \n",
    "    for artifact in optional_artifacts:\n",
    "        if os.path.exists(artifact):\n",
    "            artifacts.append(artifact)\n",
    "    \n",
    "    # Filter to only existing artifacts\n",
    "    existing_artifacts = [a for a in artifacts if os.path.exists(a)]\n",
    "    \n",
    "    print(f\"Results summary: {results}\")\n",
    "    print(f\"Artifacts ({len(existing_artifacts)}): {existing_artifacts}\")\n",
    "    \n",
    "    # Finalize notebook with enhanced metadata\n",
    "    metadata = notebook_finalize('catalog_comparison', results, existing_artifacts)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Catalog Comparison Notebook Completed Successfully!\")\n",
    "    \n",
    "    if 'summary' in comparison_results:\n",
    "        summary = comparison_results['summary']\n",
    "        print(f\"\\nüéä Final Performance Summary:\")\n",
    "        print(f\"   üìä F1 Score: {summary.get('f1_score', 0.0):.3f}\")\n",
    "        print(f\"   üéØ Precision: {summary.get('precision', 0.0):.3f}\")\n",
    "        print(f\"   üìà Recall: {summary.get('recall', 0.0):.3f}\")\n",
    "        print(f\"   üìÅ Total Artifacts: {len(existing_artifacts)}\")\n",
    "    \n",
    "    print(f\"\\nüìñ View the detailed comparison report: {output_base}/comparison_report.html\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error finalizing notebook: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Ensure we have basic artifacts for pipeline continuation\n",
    "    basic_results = {'comparison_completed': False, 'error': str(e)}\n",
    "    basic_artifacts = [f for f in [f\"{output_base}/performance_metrics.json\"] if os.path.exists(f)]\n",
    "    \n",
    "    try:\n",
    "        metadata = notebook_finalize('catalog_comparison_error', basic_results, basic_artifacts)\n",
    "        print(f\"Generated error metadata for pipeline continuation\")\n",
    "    except:\n",
    "        print(f\"Failed to generate error metadata\")\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}