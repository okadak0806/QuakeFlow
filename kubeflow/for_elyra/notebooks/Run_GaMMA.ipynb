{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cab3a31",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c6184",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058612bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T04:34:59.532109Z",
     "iopub.status.busy": "2025-06-03T04:34:59.531479Z",
     "iopub.status.idle": "2025-06-03T04:34:59.867838Z",
     "shell.execute_reply": "2025-06-03T04:34:59.866819Z"
    },
    "papermill": {
     "duration": 0.339197,
     "end_time": "2025-06-03T04:34:59.868867",
     "exception": true,
     "start_time": "2025-06-03T04:34:59.529670",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gamma.utils import association, convert_picks_csv, from_seconds\n",
    "from pyproj import Proj\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043de46c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_i=0\n",
    "index_json='config/index.json'\n",
    "config_json='config/config.json'\n",
    "pick_csv='phasenet/picks.csv'\n",
    "station_json='stations/stations.json'\n",
    "\n",
    "gamma_dir = \"gamma\"\n",
    "if not os.path.exists(gamma_dir):\n",
    "    os.makedirs(gamma_dir)\n",
    "\n",
    "gamma_catalog_csv=f\"gamma/catalog_{node_i:03d}.csv\"\n",
    "gamma_pick_csv=f\"gamma/picks_{node_i:03d}.csv\"\n",
    "\n",
    "bucket_name=\"catalogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bac77b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "catalog_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "if not os.path.exists(catalog_dir):\n",
    "    os.makedirs(catalog_dir)\n",
    "\n",
    "## read config\n",
    "with open(index_json, \"r\") as fp:\n",
    "    index = json.load(fp)\n",
    "idx = index[node_i]\n",
    "\n",
    "with open(config_json, \"r\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "## read picks\n",
    "picks = pd.read_csv(pick_csv, parse_dates=[\"phase_time\"])\n",
    "picks[\"id\"] = picks[\"station_id\"]\n",
    "picks[\"timestamp\"] = picks[\"phase_time\"]\n",
    "picks[\"amp\"] = picks[\"phase_amp\"]\n",
    "picks[\"type\"] = picks[\"phase_type\"]\n",
    "picks[\"prob\"] = picks[\"phase_score\"]\n",
    "\n",
    "## read stations\n",
    "with open(station_json, \"r\") as fp:\n",
    "    stations = json.load(fp)\n",
    "stations = pd.DataFrame.from_dict(stations, orient=\"index\")\n",
    "stations[\"id\"] = stations.index\n",
    "proj = Proj(f\"+proj=sterea +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\n",
    "stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n",
    "    lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n",
    ")\n",
    "stations[\"z(km)\"] = stations[\"elevation(m)\"].apply(lambda x: -x / 1e3)\n",
    "\n",
    "## setting GMMA configs\n",
    "config[\"use_dbscan\"] = True\n",
    "config[\"use_amplitude\"] = True\n",
    "config[\"method\"] = \"BGMM\"\n",
    "if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n",
    "    config[\"oversample_factor\"] = 4\n",
    "if config[\"method\"] == \"GMM\":  ## GaussianMixture\n",
    "    config[\"oversample_factor\"] = 1\n",
    "\n",
    "# Earthquake location\n",
    "config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n",
    "config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.73}\n",
    "config[\"x(km)\"] = (np.array(config[\"xlim_degree\"]) - np.array(config[\"center\"][0])) * config[\"degree2km\"]\n",
    "config[\"y(km)\"] = (np.array(config[\"ylim_degree\"]) - np.array(config[\"center\"][1])) * config[\"degree2km\"]\n",
    "config[\"z(km)\"] = (0, 60)\n",
    "config[\"bfgs_bounds\"] = (\n",
    "    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n",
    "    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n",
    "    (0, config[\"z(km)\"][1] + 1),  # z\n",
    "    (None, None),  # t\n",
    ")\n",
    "\n",
    "# DBSCAN\n",
    "config[\"dbscan_eps\"] = 10  # second\n",
    "config[\"dbscan_min_samples\"] = 3  ## see DBSCAN\n",
    "\n",
    "# Filtering\n",
    "config[\"min_picks_per_eq\"] = min(10, len(stations) // 2)\n",
    "config[\"min_p_picks_per_eq\"] = 0\n",
    "config[\"min_s_picks_per_eq\"] = 0\n",
    "config[\"max_sigma11\"] = 2.0  # s\n",
    "config[\"max_sigma22\"] = 2.0  # m/s\n",
    "config[\"max_sigma12\"] = 1.0  # covariance\n",
    "\n",
    "# if use amplitude\n",
    "if config[\"use_amplitude\"]:\n",
    "    picks = picks[picks[\"amp\"] != -1]\n",
    "\n",
    "# print(config)\n",
    "for k, v in config.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "## run GMMA association\n",
    "event_idx0 = 1\n",
    "assignments = []\n",
    "catalogs, assignments = association(picks, stations, config, event_idx0, method=config[\"method\"])\n",
    "event_idx0 += len(catalogs)\n",
    "\n",
    "## create catalog\n",
    "catalogs = pd.DataFrame(\n",
    "    catalogs,\n",
    "    columns=[\"time\"]\n",
    "    + config[\"dims\"]\n",
    "    + [\n",
    "        \"magnitude\",\n",
    "        \"sigma_time\",\n",
    "        \"sigma_amp\",\n",
    "        \"cov_time_amp\",\n",
    "        \"event_index\",\n",
    "        \"gamma_score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "catalogs[[\"longitude\", \"latitude\"]] = catalogs.apply(\n",
    "    lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)),\n",
    "    axis=1,\n",
    ")\n",
    "catalogs[\"depth(m)\"] = catalogs[\"z(km)\"].apply(lambda x: x * 1e3)\n",
    "\n",
    "catalogs.sort_values(by=[\"time\"], inplace=True)\n",
    "with open(gamma_catalog_csv, \"w\") as fp:\n",
    "    catalogs.to_csv(\n",
    "        fp,\n",
    "        index=False,\n",
    "        float_format=\"%.3f\",\n",
    "        date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "        columns=[\n",
    "            \"time\",\n",
    "            \"magnitude\",\n",
    "            \"longitude\",\n",
    "            \"latitude\",\n",
    "            \"depth(m)\",\n",
    "            \"sigma_time\",\n",
    "            \"sigma_amp\",\n",
    "            \"cov_time_amp\",\n",
    "            \"gamma_score\",\n",
    "            \"event_index\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "## add assignment to picks\n",
    "assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n",
    "picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n",
    "picks.sort_values(by=[\"timestamp\"], inplace=True)\n",
    "with open(gamma_pick_csv, \"w\") as fp:\n",
    "    picks.to_csv(\n",
    "        fp,\n",
    "        index=False,\n",
    "        date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "        columns=[\n",
    "            \"station_id\",\n",
    "            \"phase_time\",\n",
    "            \"phase_type\",\n",
    "            \"phase_score\",\n",
    "            \"phase_amp\",\n",
    "            \"gamma_score\",\n",
    "            \"event_index\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# 出力ファイルの存在確認\n",
    "if os.path.exists(gamma_catalog_csv) and os.path.exists(gamma_pick_csv):\n",
    "    print(f\"Successfully created output files for node {node_i}\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to create output files for node {node_i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ed324",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_csv = \"gamma/gamma_picks.csv\"\n",
    "catalog_csv = \"gamma/gamma_catalog.csv\"\n",
    "\n",
    "with open(index_json, \"r\") as fp:\n",
    "    index = json.load(fp)\n",
    "    \n",
    "with open(config_json, \"r\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# files_catalog = sorted(glob(tmp_path(\"catalog_*.csv\")))\n",
    "# files_picks = sorted(glob(tmp_path(\"picks_*.csv\")))\n",
    "files_catalog = [f\"gamma/catalog_{node_i:03d}.csv\" for node_i in range(len(index))]\n",
    "files_picks = [f\"gamma/picks_{node_i:03d}.csv\" for node_i in range(len(index))]\n",
    "print(f\"Merge catalog: {files_catalog}\")\n",
    "print(f\"Merge picks: {files_picks}\")\n",
    "\n",
    "if len(files_catalog) > 0:\n",
    "    catalog_list = []\n",
    "    for f in files_catalog:\n",
    "        tmp = pd.read_csv(f, dtype=str)\n",
    "        tmp[\"file_index\"] = f.rstrip(\".csv\").split(\"_\")[-1]\n",
    "        catalog_list.append(tmp)\n",
    "    merged_catalog = pd.concat(catalog_list).sort_values(by=\"time\")\n",
    "    merged_catalog[\"match_id\"] = merged_catalog.apply(lambda x: f'{x[\"event_index\"]}_{x[\"file_index\"]}', axis=1)\n",
    "    merged_catalog.sort_values(by=\"time\", inplace=True, ignore_index=True)\n",
    "    merged_catalog.drop(columns=[\"event_index\", \"file_index\"], inplace=True)\n",
    "    merged_catalog[\"event_index\"] = merged_catalog.index.values + 1\n",
    "    mapping = dict(zip(merged_catalog[\"match_id\"], merged_catalog[\"event_index\"]))\n",
    "    merged_catalog.drop(columns=[\"match_id\"], inplace=True)\n",
    "    merged_catalog.to_csv(catalog_csv, index=False)\n",
    "    del merged_catalog\n",
    "\n",
    "    pick_list = []\n",
    "    for f in files_picks:\n",
    "        tmp = pd.read_csv(f, dtype=str)\n",
    "        tmp[\"file_index\"] = f.rstrip(\".csv\").split(\"_\")[-1]\n",
    "        pick_list.append(tmp)\n",
    "    merged_picks = pd.concat(pick_list).sort_values(by=\"phase_time\")\n",
    "    merged_picks[\"match_id\"] = merged_picks.apply(lambda x: f'{x[\"event_index\"]}_{x[\"file_index\"]}', axis=1)\n",
    "    merged_picks.drop(columns=[\"event_index\", \"file_index\"], inplace=True)\n",
    "    merged_picks[\"event_index\"] = merged_picks[\"match_id\"].apply(lambda x: mapping[x] if x in mapping else -1)\n",
    "    merged_picks.drop(columns=[\"match_id\"], inplace=True)\n",
    "    merged_picks.to_csv(picks_csv, index=False)\n",
    "    del merged_picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db593f74",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kubeflow Pipelines UI用のメタデータ出力\n",
    "if os.environ.get('ELYRA_RUNTIME_ENV') == 'kfp':\n",
    "    # For information about Elyra environment variables refer to\n",
    "    # https://elyra.readthedocs.io/en/stable/user_guide/best-practices-file-based-nodes.html#proprietary-environment-variables\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': f'# Run GaMMA Complete\\n',\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open('mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.227813,
   "end_time": "2025-06-03T04:35:00.187214",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/okadak/QuakeFlow/kubeflow/for_elyra/notebooks/Run_GaMMA.ipynb",
   "output_path": "/home/okadak/QuakeFlow/kubeflow/for_elyra/notebooks/Run_GaMMA.ipynb",
   "parameters": {},
   "start_time": "2025-06-03T04:34:58.959401",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
