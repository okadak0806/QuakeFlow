{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gamma.utils import association, convert_picks_csv, from_seconds\n",
    "from pyproj import Proj\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_i=0\n",
    "index_json='config/index.json'\n",
    "config_json='config/config.json'\n",
    "pick_csv='phasenet/piks.csv'\n",
    "station_json='staions/stations.json',\n",
    "gamma_catalog_csv=f\"gamma/catalog_{node_i:03d}.csv\"\n",
    "gamma_pick_csv=f\"gamma/pics_{node_i:03d}.csv\"\n",
    "bucket_name=\"catalogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "if not os.path.exists(catalog_dir):\n",
    "    os.makedirs(catalog_dir)\n",
    "\n",
    "## read config\n",
    "with open(index_json, \"r\") as fp:\n",
    "    index = json.load(fp)\n",
    "idx = index[node_i]\n",
    "\n",
    "with open(config_json, \"r\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "## read picks\n",
    "# picks = pd.read_json(pick_json)\n",
    "picks = pd.read_csv(pick_csv, parse_dates=[\"phase_time\"])\n",
    "picks[\"id\"] = picks[\"station_id\"]\n",
    "picks[\"timestamp\"] = picks[\"phase_time\"]\n",
    "picks[\"amp\"] = picks[\"phase_amp\"]\n",
    "picks[\"type\"] = picks[\"phase_type\"]\n",
    "picks[\"prob\"] = picks[\"phase_score\"]\n",
    "\n",
    "## read stations\n",
    "# stations = pd.read_csv(station_csv, delimiter=\"\\t\")\n",
    "with open(station_json, \"r\") as fp:\n",
    "    stations = json.load(fp)\n",
    "stations = pd.DataFrame.from_dict(stations, orient=\"index\")\n",
    "# stations = stations.rename(columns={\"station\": \"id\"})\n",
    "stations[\"id\"] = stations.index\n",
    "proj = Proj(f\"+proj=sterea +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\n",
    "stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n",
    "    lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n",
    ")\n",
    "stations[\"z(km)\"] = stations[\"elevation(m)\"].apply(lambda x: -x / 1e3)\n",
    "\n",
    "## setting GMMA configs\n",
    "config[\"use_dbscan\"] = True\n",
    "config[\"use_amplitude\"] = True\n",
    "config[\"method\"] = \"BGMM\"\n",
    "if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n",
    "    config[\"oversample_factor\"] = 4\n",
    "if config[\"method\"] == \"GMM\":  ## GaussianMixture\n",
    "    config[\"oversample_factor\"] = 1\n",
    "\n",
    "# Earthquake location\n",
    "config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n",
    "config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.73}\n",
    "config[\"x(km)\"] = (np.array(config[\"xlim_degree\"]) - np.array(config[\"center\"][0])) * config[\"degree2km\"]\n",
    "config[\"y(km)\"] = (np.array(config[\"ylim_degree\"]) - np.array(config[\"center\"][1])) * config[\"degree2km\"]\n",
    "config[\"z(km)\"] = (0, 60)\n",
    "config[\"bfgs_bounds\"] = (\n",
    "    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n",
    "    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n",
    "    (0, config[\"z(km)\"][1] + 1),  # z\n",
    "    (None, None),  # t\n",
    ")\n",
    "\n",
    "# DBSCAN\n",
    "config[\"dbscan_eps\"] = 10  # second\n",
    "config[\"dbscan_min_samples\"] = 3  ## see DBSCAN\n",
    "\n",
    "# Filtering\n",
    "config[\"min_picks_per_eq\"] = min(10, len(stations) // 2)\n",
    "config[\"min_p_picks_per_eq\"] = 0\n",
    "config[\"min_s_picks_per_eq\"] = 0\n",
    "config[\"max_sigma11\"] = 2.0  # s\n",
    "config[\"max_sigma22\"] = 2.0  # m/s\n",
    "config[\"max_sigma12\"] = 1.0  # covariance\n",
    "\n",
    "# if use amplitude\n",
    "if config[\"use_amplitude\"]:\n",
    "    picks = picks[picks[\"amp\"] != -1]\n",
    "\n",
    "# print(config)\n",
    "for k, v in config.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "## run GMMA association\n",
    "event_idx0 = 1\n",
    "assignments = []\n",
    "catalogs, assignments = association(picks, stations, config, event_idx0, method=config[\"method\"])\n",
    "event_idx0 += len(catalogs)\n",
    "\n",
    "## create catalog\n",
    "catalogs = pd.DataFrame(\n",
    "    catalogs,\n",
    "    columns=[\"time\"]\n",
    "    + config[\"dims\"]\n",
    "    + [\n",
    "        \"magnitude\",\n",
    "        \"sigma_time\",\n",
    "        \"sigma_amp\",\n",
    "        \"cov_time_amp\",\n",
    "        \"event_index\",\n",
    "        \"gamma_score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "catalogs[[\"longitude\", \"latitude\"]] = catalogs.apply(\n",
    "    lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)),\n",
    "    axis=1,\n",
    ")\n",
    "catalogs[\"depth(m)\"] = catalogs[\"z(km)\"].apply(lambda x: x * 1e3)\n",
    "\n",
    "catalogs.sort_values(by=[\"time\"], inplace=True)\n",
    "with open(gamma_catalog_csv, \"w\") as fp:\n",
    "    catalogs.to_csv(\n",
    "        fp,\n",
    "        # sep=\"\\t\",\n",
    "        index=False,\n",
    "        float_format=\"%.3f\",\n",
    "        date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "        columns=[\n",
    "            \"time\",\n",
    "            \"magnitude\",\n",
    "            \"longitude\",\n",
    "            \"latitude\",\n",
    "            \"depth(m)\",\n",
    "            \"sigma_time\",\n",
    "            \"sigma_amp\",\n",
    "            \"cov_time_amp\",\n",
    "            \"gamma_score\",\n",
    "            \"event_index\",\n",
    "        ],\n",
    "    )\n",
    "# catalogs = catalogs[\n",
    "#     ['time', 'magnitude', 'longitude', 'latitude', 'depth(m)', 'sigma_time', 'sigma_amp']\n",
    "# ]\n",
    "\n",
    "## add assignment to picks\n",
    "assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n",
    "picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n",
    "picks.sort_values(by=[\"timestamp\"], inplace=True)\n",
    "with open(gamma_pick_csv, \"w\") as fp:\n",
    "    picks.to_csv(\n",
    "        fp,\n",
    "        # sep=\"\\t\",\n",
    "        index=False,\n",
    "        date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "        columns=[\n",
    "            \"station_id\",\n",
    "            \"phase_time\",\n",
    "            \"phase_type\",\n",
    "            \"phase_score\",\n",
    "            \"phase_amp\",\n",
    "            \"gamma_score\",\n",
    "            \"event_index\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "## upload to s3 bucket\n",
    "# try:\n",
    "#     from minio import Minio\n",
    "\n",
    "#     minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "#     if not minioClient.bucket_exists(bucket_name):\n",
    "#         minioClient.make_bucket(bucket_name)\n",
    "\n",
    "#     minioClient.fput_object(\n",
    "#         bucket_name,\n",
    "#         f\"{config['region']}/gamma/catalog_{node_i:03d}.csv\",\n",
    "#         gamma_catalog_csv,\n",
    "#     )\n",
    "\n",
    "#     minioClient.fput_object(\n",
    "#         bucket_name,\n",
    "#         f\"{config['region']}/gamma/picks_{node_i:03d}.csv\",\n",
    "#         gamma_pick_csv,\n",
    "#     )\n",
    "\n",
    "# except Exception as err:\n",
    "#     print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "#     pass\n",
    "\n",
    "## upload to mongodb\n",
    "# try:\n",
    "#     from pymongo import MongoClient\n",
    "\n",
    "#     username = \"root\"\n",
    "#     password = \"quakeflow123\"\n",
    "#     client = MongoClient(f\"mongodb://{username}:{password}@127.0.0.1:27017\")\n",
    "#     db = client[\"quakeflow\"]\n",
    "#     collection = db[\"waveform\"]\n",
    "#     for i, p in tqdm(picks.iterrows(), desc=\"Uploading to mongodb\"):\n",
    "#         collection.update(\n",
    "#             {\"_id\": f\"{p['station_id']}_{p['timestamp'].isoformat(timespec='milliseconds')}_{p['type']}\"},\n",
    "#             {\"$set\": {\"event_index\": p[\"event_index\"]}},\n",
    "#         )\n",
    "# except Exception as err:\n",
    "#     print(f\"ERROR: can not access mongodb service! \\n{err}\")\n",
    "#     pass\n",
    "\n",
    "# return f\"catalog_{node_i:03d}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow Pipelines UI用のメタデータ出力\n",
    "if os.environ.get('ELYRA_RUNTIME_ENV') == 'kfp':\n",
    "    # For information about Elyra environment variables refer to\n",
    "    # https://elyra.readthedocs.io/en/stable/user_guide/best-practices-file-based-nodes.html#proprietary-environment-variables\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': f'# Run GaMMA Complete\\n',\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open('mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quakeflow1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
