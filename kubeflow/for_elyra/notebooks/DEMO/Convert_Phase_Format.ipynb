{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c68952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバッグ: 現在の環境とファイルを確認\n",
    "import os\n",
    "print(\"=== Environment Debug ===\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Directory contents: {os.listdir('.')}\")\n",
    "\n",
    "# サブディレクトリも確認\n",
    "for item in os.listdir('.'):\n",
    "    if os.path.isdir(item):\n",
    "        print(f\"Directory {item}/: {os.listdir(item)}\")\n",
    "\n",
    "print(\"\\n=== Environment Variables ===\")\n",
    "import os\n",
    "for key, value in os.environ.items():\n",
    "    if 'ELYRA' in key or 'KFP' in key or 'KUBEFLOW' in key:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_parallelファイルを探す\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=== Looking for num_parallel.txt ===\")\n",
    "# 現在のディレクトリで探す\n",
    "if os.path.exists('num_parallel.txt'):\n",
    "    with open('num_parallel.txt', 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        print(f\"Found num_parallel.txt: {content}\")\n",
    "        try:\n",
    "            nodes = [int(x) for x in content.strip('[]').split(',')]\n",
    "        except:\n",
    "            nodes = [0]  # デフォルト\n",
    "else:\n",
    "    # グロブパターンで探す\n",
    "    files = glob.glob('**/num_parallel.txt', recursive=True)\n",
    "    print(f\"Searching recursively: {files}\")\n",
    "    if files:\n",
    "        with open(files[0], 'r') as f:\n",
    "            content = f.read().strip()\n",
    "            nodes = [int(x) for x in content.strip('[]').split(',')]\n",
    "    else:\n",
    "        print(\"num_parallel.txt not found, using default\")\n",
    "        nodes = [0]\n",
    "\n",
    "print(f\"Using nodes: {nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_phase(\n",
    "    node_i: int,\n",
    "    config_json: str,\n",
    "    picks_csv: str,\n",
    "    catalog_csv: str,\n",
    "    hypodd_phase:str,\n",
    "    data_path: str,\n",
    "    # bucket_name: str = \"catalogs\",\n",
    "    # s3_url: str = \"minio-service:9000\",\n",
    "    # secure: bool = False,\n",
    "):\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    hypodd_path = os.path.join(data_path, \"hypodd\")\n",
    "    if not os.path.exists(hypodd_path):\n",
    "        os.mkdir(hypodd_path)\n",
    "\n",
    "    picks = pd.read_csv(picks_csv)\n",
    "    events = pd.read_csv(catalog_csv)\n",
    "\n",
    "    if \"MAXEVENT\" in config[\"hypodd\"]:\n",
    "        MAXEVENT = config[\"hypodd\"][\"MAXEVENT\"]\n",
    "    else:\n",
    "        MAXEVENT = 1e4  ## segment by time\n",
    "    MAXEVENT = len(events) // ((len(events) - 1) // MAXEVENT + 1) + 1\n",
    "    num_parallel = int((len(events) - 1) // MAXEVENT + 1)\n",
    "\n",
    "    events.sort_values(\"time\", inplace=True)\n",
    "    events = events.iloc[node_i::num_parallel]\n",
    "    picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]\n",
    "    # output_lines = []\n",
    "    output_file = open(hypodd_phase, \"w\")\n",
    "\n",
    "    picks_by_event = picks.groupby(\"event_index\").groups\n",
    "    # for i in tqdm(range(node_i, len(events), num_parallel)):\n",
    "    #     event = events.iloc[i]\n",
    "    for i, event in events.iterrows():\n",
    "        event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        lat = event[\"latitude\"]\n",
    "        lng = event[\"longitude\"]\n",
    "        dep = event[\"depth(m)\"] / 1e3\n",
    "        mag = event[\"magnitude\"]\n",
    "        EH = 0\n",
    "        EZ = 0\n",
    "        RMS = event[\"sigma_time\"]\n",
    "\n",
    "        year, month, day, hour, min, sec = (\n",
    "            event_time.year,\n",
    "            event_time.month,\n",
    "            event_time.day,\n",
    "            event_time.hour,\n",
    "            event_time.minute,\n",
    "            float(event_time.strftime(\"%S.%f\")),\n",
    "        )\n",
    "        event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"\n",
    "\n",
    "        # output_lines.append(event_line)\n",
    "        output_file.write(event_line)\n",
    "\n",
    "        picks_idx = picks_by_event[event[\"event_index\"]]\n",
    "        for j in picks_idx:\n",
    "            # pick = picks.iloc[j]\n",
    "            pick = picks.loc[j]\n",
    "            network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")\n",
    "            phase_type = pick[\"phase_type\"].upper()\n",
    "            phase_score = pick[\"phase_score\"]\n",
    "            pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()\n",
    "            tmp_code = f\"{station_code}{channel_code}\"\n",
    "            pick_line = f\"{tmp_code:<7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"\n",
    "            # output_lines.append(pick_line)\n",
    "            output_file.write(pick_line)\n",
    "\n",
    "    # with open(hypodd_phase, \"w\") as fp:\n",
    "    #     fp.writelines(output_lines)\n",
    "    \n",
    "    return hypodd_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なファイルを探す\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def find_file(filename):\n",
    "    # 現在のディレクトリ\n",
    "    if os.path.exists(filename):\n",
    "        return filename\n",
    "    \n",
    "    # サブディレクトリを再帰的に検索\n",
    "    files = glob.glob(f'**/{filename}', recursive=True)\n",
    "    if files:\n",
    "        return files[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ファイルを探す\n",
    "config_file = find_file('config.json') or find_file('config/config.json')\n",
    "picks_file = find_file('gamma_picks.csv') or find_file('gamma/gamma_picks.csv')\n",
    "catalog_file = find_file('gamma_catalog.csv') or find_file('gamma/gamma_catalog.csv')\n",
    "\n",
    "print(f\"Found files:\")\n",
    "print(f\"  config: {config_file}\")\n",
    "print(f\"  picks: {picks_file}\")\n",
    "print(f\"  catalog: {catalog_file}\")\n",
    "\n",
    "if all([config_file, picks_file, catalog_file]):\n",
    "    for node_i in nodes:\n",
    "        convert_phase(\n",
    "            node_i,\n",
    "            config_file,\n",
    "            picks_file,\n",
    "            catalog_file,\n",
    "            f\"hypodd_phase_{node_i:03d}.pha\",\n",
    "            \"./\",\n",
    "        )\n",
    "else:\n",
    "    print(\"ERROR: Required files not found!\")\n",
    "    missing = []\n",
    "    if not config_file: missing.append('config.json')\n",
    "    if not picks_file: missing.append('gamma_picks.csv')\n",
    "    if not catalog_file: missing.append('gamma_catalog.csv')\n",
    "    print(f\"Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaab682",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('num_parallel.txt', 'r') as f:\n",
    "    nodes = [int(x) for x in f.read().strip('[]').split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82644a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def find_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return filename\n",
    "    files = glob.glob(f'**/{filename}', recursive=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "# 正しいパスを使用\n",
    "config_file = find_file('config.json') or \"config/config.json\"\n",
    "picks_file = find_file('gamma_picks.csv') or \"gamma/gamma_picks.csv\" \n",
    "catalog_file = find_file('gamma_catalog.csv') or \"gamma/gamma_catalog.csv\"\n",
    "\n",
    "# 各nodeの処理\n",
    "generated_files = []\n",
    "for node_i in nodes:\n",
    "    output_file = f\"hypodd_phase_{node_i:03d}.pha\"\n",
    "    convert_phase(\n",
    "        node_i,\n",
    "        config_file,\n",
    "        picks_file,\n",
    "        catalog_file,\n",
    "        output_file,\n",
    "        \"./\",\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        generated_files.append(output_file)\n",
    "        print(f\"Created: {output_file}\")\n",
    "\n",
    "# 複数ファイルを1つにマージ\n",
    "print(f\"Merging {len(generated_files)} files into hypodd_phase.pha\")\n",
    "with open('hypodd/hypodd_phase.pha', 'w') as merged_file:\n",
    "    for phase_file in generated_files:\n",
    "        print(f\"Adding {phase_file}\")\n",
    "        with open(phase_file, 'r') as f:\n",
    "            merged_file.write(f.read())\n",
    "        # 個別ファイルを削除（オプション）\n",
    "        # os.remove(phase_file)\n",
    "\n",
    "print(\"Merged file created: hypodd_phase.pha\")\n",
    "\n",
    "# ファイルサイズを確認\n",
    "if os.path.exists('hypodd_phase.pha'):\n",
    "    size = os.path.getsize('hypodd_phase.pha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f2fcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T02:33:27.435238Z",
     "iopub.status.busy": "2025-06-03T02:33:27.434883Z",
     "iopub.status.idle": "2025-06-03T02:33:27.438600Z",
     "shell.execute_reply": "2025-06-03T02:33:27.437997Z"
    },
    "papermill": {
     "duration": 0.006845,
     "end_time": "2025-06-03T02:33:27.439654",
     "exception": false,
     "start_time": "2025-06-03T02:33:27.432809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kubeflow Pipelines UI用のメタデータ出力\n",
    "if os.environ.get('ELYRA_RUNTIME_ENV') == 'kfp':\n",
    "    # For information about Elyra environment variables refer to\n",
    "    # https://elyra.readthedocs.io/en/stable/user_guide/best-practices-file-based-nodes.html#proprietary-environment-variables\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': f'# Convert Station Format Complete\\n...',\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open('mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.969604,
   "end_time": "2025-06-03T02:33:27.556863",
   "environment_variables": {},
   "exception": null,
   "input_path": "Set_Config.ipynb",
   "output_path": "Set_Config-output.ipynb",
   "parameters": {},
   "start_time": "2025-06-03T02:33:26.587259",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
