{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PhaseNet Fine-Tuning for Japanese Earthquake Data\n",
    "\n",
    "**Advanced model fine-tuning using SeisBench PyTorch framework**\n",
    "\n",
    "This notebook implements fine-tuning of SeisBench PhaseNet models specifically for Japanese earthquake characteristics, including:\n",
    "- Subduction zone seismicity patterns\n",
    "- Hi-net data preprocessing\n",
    "- Deep earthquake phase characteristics\n",
    "- Regional velocity structure considerations\n",
    "\n",
    "## Features\n",
    "- Transfer learning from pre-trained PhaseNet models\n",
    "- Japanese earthquake-specific data augmentation\n",
    "- Advanced training metrics and validation\n",
    "- Model performance evaluation\n",
    "- Automated hyperparameter optimization\n",
    "- GPU-accelerated training\n",
    "\n",
    "## Inputs\n",
    "- Labeled Japanese earthquake dataset\n",
    "- Pre-trained SeisBench PhaseNet model\n",
    "- Training configuration\n",
    "- Validation dataset\n",
    "\n",
    "## Outputs\n",
    "- Fine-tuned PhaseNet model\n",
    "- Training metrics and plots\n",
    "- Model validation results\n",
    "- Performance comparison reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# SeisBench and seismology libraries\n",
    "import seisbench\n",
    "import seisbench.models as sbm\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "# ObsPy for seismic data\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# Hyperparameter optimization\n",
    "try:\n",
    "    import optuna\n",
    "    HAS_OPTUNA = True\n",
    "except ImportError:\n",
    "    HAS_OPTUNA = False\n",
    "    print(\"‚ö†Ô∏è Optuna not available - hyperparameter optimization disabled\")\n",
    "\n",
    "# Add common utilities to path\n",
    "sys.path.append('/app/examples')\n",
    "from common import (\n",
    "    notebook_setup, notebook_finalize,\n",
    "    RegionConfig, DataIO,\n",
    "    SeisbenchPhaseNetProcessor\n",
    ")\n",
    "\n",
    "print(f\"üéØ PhaseNet Fine-Tuning for Japanese Data - Starting...\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"   SeisBench version: {seisbench.__version__}\")\n",
    "print(f\"   Optuna available: {HAS_OPTUNA}\")\n",
    "print(f\"   Notebook start time: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize environment and configuration\n",
    "start_time = datetime.now()\n",
    "config, region_name, elyra_env = notebook_setup(\n",
    "    default_region=\"japan\",\n",
    "    required_inputs=[\n",
    "        \"training_data/\",  # Japanese labeled dataset\n",
    "        \"config/config.json\"\n",
    "    ],\n",
    "    outputs_dir=\"fine_tuned_models\"\n",
    ")\n",
    "\n",
    "print(f\"üéØ Fine-Tuning Configuration:\")\n",
    "print(f\"   Region: {region_name}\")\n",
    "print(f\"   Device preference: {config.config.get('fine_tuning', {}).get('device', 'auto')}\")\n",
    "print(f\"   Base model: {config.config.get('fine_tuning', {}).get('base_model', 'phasenet_original')}\")\n",
    "print(f\"   Epochs: {config.config.get('fine_tuning', {}).get('epochs', 20)}\")\n",
    "print(f\"   Batch size: {config.config.get('fine_tuning', {}).get('batch_size', 16)}\")\n",
    "print(f\"   Learning rate: {config.config.get('fine_tuning', {}).get('learning_rate', 1e-4)}\")\n",
    "\n",
    "# Training parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = config.config.get('fine_tuning', {}).get('epochs', 20)\n",
    "BATCH_SIZE = config.config.get('fine_tuning', {}).get('batch_size', 16)\n",
    "LEARNING_RATE = config.config.get('fine_tuning', {}).get('learning_rate', 1e-4)\n",
    "BASE_MODEL = config.config.get('fine_tuning', {}).get('base_model', 'phasenet_original')\n",
    "VALIDATION_SPLIT = config.config.get('fine_tuning', {}).get('validation_split', 0.2)\n",
    "\n",
    "# Create output directories\n",
    "output_dir = \"fine_tuned_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/metrics\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/plots\", exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"üéØ Training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class for Japanese earthquake data\n",
    "class JapaneseEarthquakeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for Japanese earthquake waveforms with phase labels.\n",
    "    \n",
    "    Supports various data formats and includes Japan-specific preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transforms=None, sampling_rate=100, window_length=30):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transforms = transforms\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.window_length = window_length\n",
    "        self.window_samples = int(window_length * sampling_rate)\n",
    "        \n",
    "        # Load data index\n",
    "        self.samples = self._load_data_index()\n",
    "        print(f\"üìä Loaded {len(self.samples)} training samples\")\n",
    "    \n",
    "    def _load_data_index(self):\n",
    "        \"\"\"Load data index from various possible formats.\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Check for metadata file\n",
    "        metadata_files = [\n",
    "            self.data_dir / \"metadata.csv\",\n",
    "            self.data_dir / \"index.csv\",\n",
    "            self.data_dir / \"samples.json\"\n",
    "        ]\n",
    "        \n",
    "        for metadata_file in metadata_files:\n",
    "            if metadata_file.exists():\n",
    "                print(f\"üìã Loading metadata from: {metadata_file}\")\n",
    "                \n",
    "                if metadata_file.suffix == '.csv':\n",
    "                    df = pd.read_csv(metadata_file)\n",
    "                    for _, row in df.iterrows():\n",
    "                        samples.append({\n",
    "                            'waveform_file': self.data_dir / row['waveform_file'],\n",
    "                            'p_pick_sample': row.get('p_pick_sample', -1),\n",
    "                            's_pick_sample': row.get('s_pick_sample', -1),\n",
    "                            'station': row.get('station', 'unknown'),\n",
    "                            'event_id': row.get('event_id', 'unknown')\n",
    "                        })\n",
    "                elif metadata_file.suffix == '.json':\n",
    "                    with open(metadata_file) as f:\n",
    "                        data = json.load(f)\n",
    "                    for sample in data:\n",
    "                        samples.append(sample)\n",
    "                break\n",
    "        \n",
    "        # If no metadata found, create dummy dataset for demonstration\n",
    "        if not samples:\n",
    "            print(f\"‚ö†Ô∏è No labeled data found - creating synthetic dataset for demonstration\")\n",
    "            samples = self._create_synthetic_dataset()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _create_synthetic_dataset(self, n_samples=100):\n",
    "        \"\"\"Create synthetic dataset for demonstration purposes.\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Create synthetic sample metadata\n",
    "            p_pick = np.random.randint(300, 800)  # P-pick between 3-8 seconds\n",
    "            s_pick = p_pick + np.random.randint(100, 400)  # S-pick 1-4 seconds after P\n",
    "            \n",
    "            samples.append({\n",
    "                'synthetic': True,\n",
    "                'sample_id': i,\n",
    "                'p_pick_sample': p_pick,\n",
    "                's_pick_sample': s_pick,\n",
    "                'station': f'SYN{i:03d}',\n",
    "                'event_id': f'synthetic_{i}'\n",
    "            })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        if sample.get('synthetic', False):\n",
    "            # Generate synthetic waveform\n",
    "            waveform, labels = self._generate_synthetic_waveform(sample)\n",
    "        else:\n",
    "            # Load real waveform\n",
    "            waveform, labels = self._load_real_waveform(sample)\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transforms:\n",
    "            waveform = self.transforms(waveform)\n",
    "        \n",
    "        return {\n",
    "            'waveform': torch.tensor(waveform, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float32),\n",
    "            'metadata': sample\n",
    "        }\n",
    "    \n",
    "    def _generate_synthetic_waveform(self, sample):\n",
    "        \"\"\"Generate synthetic 3-component waveform with phase arrivals.\"\"\"\n",
    "        # Create 3-component waveform (Z, N, E)\n",
    "        waveform = np.random.randn(3, self.window_samples) * 0.1\n",
    "        \n",
    "        # Add P-wave signal\n",
    "        p_sample = sample['p_pick_sample']\n",
    "        if 0 <= p_sample < self.window_samples - 50:\n",
    "            # P-wave primarily on vertical component\n",
    "            p_signal = np.exp(-0.1 * np.arange(50)) * np.sin(2 * np.pi * 8 * np.arange(50) / self.sampling_rate)\n",
    "            waveform[0, p_sample:p_sample+50] += p_signal * 2.0\n",
    "        \n",
    "        # Add S-wave signal\n",
    "        s_sample = sample['s_pick_sample']\n",
    "        if 0 <= s_sample < self.window_samples - 100:\n",
    "            # S-wave on horizontal components\n",
    "            s_signal = np.exp(-0.05 * np.arange(100)) * np.sin(2 * np.pi * 4 * np.arange(100) / self.sampling_rate)\n",
    "            waveform[1, s_sample:s_sample+100] += s_signal * 1.5\n",
    "            waveform[2, s_sample:s_sample+100] += s_signal * 1.2\n",
    "        \n",
    "        # Create labels (P, S, Noise probabilities)\n",
    "        labels = np.zeros((3, self.window_samples))\n",
    "        \n",
    "        # P-wave label\n",
    "        if 0 <= p_sample < self.window_samples:\n",
    "            p_width = 20\n",
    "            p_start = max(0, p_sample - p_width//2)\n",
    "            p_end = min(self.window_samples, p_sample + p_width//2)\n",
    "            labels[0, p_start:p_end] = np.exp(-0.5 * ((np.arange(p_start, p_end) - p_sample) / (p_width/4))**2)\n",
    "        \n",
    "        # S-wave label\n",
    "        if 0 <= s_sample < self.window_samples:\n",
    "            s_width = 30\n",
    "            s_start = max(0, s_sample - s_width//2)\n",
    "            s_end = min(self.window_samples, s_sample + s_width//2)\n",
    "            labels[1, s_start:s_end] = np.exp(-0.5 * ((np.arange(s_start, s_end) - s_sample) / (s_width/4))**2)\n",
    "        \n",
    "        # Noise label (1 - P - S)\n",
    "        labels[2, :] = np.maximum(0, 1 - labels[0, :] - labels[1, :])\n",
    "        \n",
    "        return waveform, labels\n",
    "    \n",
    "    def _load_real_waveform(self, sample):\n",
    "        \"\"\"Load real waveform data (implementation depends on data format).\"\"\"\n",
    "        # Placeholder for real data loading\n",
    "        print(f\"‚ö†Ô∏è Real data loading not implemented - using synthetic data\")\n",
    "        return self._generate_synthetic_waveform(sample)\n",
    "\n",
    "print(f\"‚úÖ Japanese Earthquake Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and prepare training dataset\n",
    "print(f\"üìä Loading Japanese earthquake training dataset...\")\n",
    "\n",
    "# Check if training data directory exists\n",
    "training_data_dir = \"training_data\"\n",
    "if not os.path.exists(training_data_dir):\n",
    "    print(f\"‚ö†Ô∏è Training data directory not found: {training_data_dir}\")\n",
    "    print(f\"   Creating directory and using synthetic data for demonstration\")\n",
    "    os.makedirs(training_data_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset with Japan-specific transforms\n",
    "class JapanSpecificTransforms:\n",
    "    \"\"\"Japan-specific data augmentation transforms.\"\"\"\n",
    "    \n",
    "    def __init__(self, noise_level=0.05, time_shift_max=50):\n",
    "        self.noise_level = noise_level\n",
    "        self.time_shift_max = time_shift_max\n",
    "    \n",
    "    def __call__(self, waveform):\n",
    "        # Add realistic noise\n",
    "        noise = np.random.randn(*waveform.shape) * self.noise_level\n",
    "        waveform = waveform + noise\n",
    "        \n",
    "        # Random time shift (simulate picking uncertainty)\n",
    "        if self.time_shift_max > 0:\n",
    "            shift = np.random.randint(-self.time_shift_max, self.time_shift_max + 1)\n",
    "            if shift != 0:\n",
    "                waveform = np.roll(waveform, shift, axis=-1)\n",
    "        \n",
    "        # Amplitude scaling (simulate instrument response variations)\n",
    "        scale = np.random.uniform(0.8, 1.2)\n",
    "        waveform = waveform * scale\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "# Create datasets\n",
    "transforms = JapanSpecificTransforms()\n",
    "full_dataset = JapaneseEarthquakeDataset(\n",
    "    data_dir=training_data_dir,\n",
    "    transforms=transforms,\n",
    "    sampling_rate=100,\n",
    "    window_length=30\n",
    ")\n",
    "\n",
    "# Split into training and validation\n",
    "total_size = len(full_dataset)\n",
    "val_size = int(total_size * VALIDATION_SPLIT)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset prepared:\")\n",
    "print(f\"   Total samples: {total_size}\")\n",
    "print(f\"   Training samples: {train_size}\")\n",
    "print(f\"   Validation samples: {val_size}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Display sample data\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nüìã Sample batch shape:\")\n",
    "print(f\"   Waveform: {sample_batch['waveform'].shape}\")\n",
    "print(f\"   Labels: {sample_batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load base PhaseNet model for fine-tuning\n",
    "print(f\"üîÑ Loading base PhaseNet model: {BASE_MODEL}\")\n",
    "\n",
    "try:\n",
    "    # Load pre-trained SeisBench PhaseNet model\n",
    "    model = sbm.PhaseNet.from_pretrained(BASE_MODEL)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Base model loaded successfully\")\n",
    "    print(f\"   Model type: {type(model).__name__}\")\n",
    "    print(f\"   Device: {next(model.parameters()).device}\")\n",
    "    print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Fine-tuning strategy: freeze early layers, train later layers\n",
    "    freeze_layers = config.config.get('fine_tuning', {}).get('freeze_layers', True)\n",
    "    \n",
    "    if freeze_layers:\n",
    "        print(f\"üîí Freezing early layers for transfer learning...\")\n",
    "        \n",
    "        # Freeze first few convolutional layers\n",
    "        layers_to_freeze = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'conv' in name.lower() and layers_to_freeze < 6:  # Freeze first 6 conv layers\n",
    "                param.requires_grad = False\n",
    "                layers_to_freeze += 1\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"   Trainable parameters after freezing: {trainable_params:,}\")\n",
    "        print(f\"   Frozen parameters: {sum(p.numel() for p in model.parameters()) - trainable_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading base model: {e}\")\n",
    "    print(f\"   Available models: {sbm.model_list()}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "print(f\"‚öôÔ∏è Setting up training components...\")\n",
    "\n",
    "# Loss function - weighted to handle class imbalance\n",
    "class WeightedPhaseLoss(nn.Module):\n",
    "    \"\"\"Custom loss function for phase picking with class weighting.\"\"\"\n",
    "    \n",
    "    def __init__(self, p_weight=2.0, s_weight=2.0, noise_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.p_weight = p_weight\n",
    "        self.s_weight = s_weight\n",
    "        self.noise_weight = noise_weight\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions: [batch, 3, samples]\n",
    "        # targets: [batch, 3, samples]\n",
    "        \n",
    "        loss = self.bce_loss(predictions, targets)\n",
    "        \n",
    "        # Apply class weights\n",
    "        weights = torch.tensor([self.p_weight, self.s_weight, self.noise_weight], \n",
    "                              device=predictions.device).view(1, 3, 1)\n",
    "        weighted_loss = loss * weights\n",
    "        \n",
    "        return weighted_loss.mean()\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = WeightedPhaseLoss(p_weight=3.0, s_weight=3.0, noise_weight=1.0)\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Training metrics tracking\n",
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.epoch_times = []\n",
    "    \n",
    "    def update(self, train_loss, val_loss, lr, epoch_time):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.learning_rates.append(lr)\n",
    "        self.epoch_times.append(epoch_time)\n",
    "    \n",
    "    def get_best_epoch(self):\n",
    "        return np.argmin(self.val_losses)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        metrics_dict = {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "            'epoch_times': self.epoch_times\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(metrics_dict, f, indent=2)\n",
    "\n",
    "metrics = TrainingMetrics()\n",
    "\n",
    "print(f\"‚úÖ Training components initialized:\")\n",
    "print(f\"   Loss function: WeightedPhaseLoss\")\n",
    "print(f\"   Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"   Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        waveforms = batch['waveform'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(waveforms)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Progress reporting\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == num_batches - 1:\n",
    "            print(f\"   Batch {batch_idx+1}/{num_batches}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            waveforms = batch['waveform'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            predictions = model(waveforms)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "print(f\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"üöÄ Starting fine-tuning training...\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "training_start = datetime.now()\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 7\n",
    "\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_start = datetime.now()\n",
    "        print(f\"\\nüìà Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"   Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Training\n",
    "        print(f\"   Training...\")\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        \n",
    "        # Validation\n",
    "        print(f\"   Validating...\")\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = (datetime.now() - epoch_start).total_seconds()\n",
    "        \n",
    "        # Update metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        metrics.update(train_loss, val_loss, current_lr, epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"   ‚úÖ Epoch {epoch+1} completed in {epoch_time:.1f}s\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"   Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint if best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            best_model_path = os.path.join(output_dir, 'best_model.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, val_loss, best_model_path)\n",
    "            print(f\"   üíæ New best model saved: {best_model_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(output_dir, 'checkpoints', f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)\n",
    "            print(f\"   üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "            break\n",
    "    \n",
    "    training_end = datetime.now()\n",
    "    total_training_time = (training_end - training_start).total_seconds()\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"   Total training time: {total_training_time:.1f} seconds\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"   Best epoch: {metrics.get_best_epoch() + 1}\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    metrics_path = os.path.join(output_dir, 'metrics', 'training_metrics.json')\n",
    "    metrics.save(metrics_path)\n",
    "    print(f\"   üìä Training metrics saved: {metrics_path}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n‚èπÔ∏è Training interrupted by user\")\n",
    "    # Save current state\n",
    "    interrupted_path = os.path.join(output_dir, 'interrupted_model.pth')\n",
    "    save_checkpoint(model, optimizer, epoch, val_loss, interrupted_path)\n",
    "    print(f\"   üíæ Interrupted model saved: {interrupted_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate training visualizations and analysis\n",
    "print(f\"üìä Generating training analysis and visualizations...\")\n",
    "\n",
    "if len(metrics.train_losses) > 0:\n",
    "    # Create comprehensive training plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('PhaseNet Fine-Tuning Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    epochs_range = range(1, len(metrics.train_losses) + 1)\n",
    "    \n",
    "    # 1. Training and validation loss\n",
    "    axes[0, 0].plot(epochs_range, metrics.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs_range, metrics.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].axvline(x=metrics.get_best_epoch() + 1, color='green', linestyle='--', alpha=0.7, label='Best Model')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Learning rate schedule\n",
    "    axes[0, 1].plot(epochs_range, metrics.learning_rates, 'g-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Learning Rate')\n",
    "    axes[0, 1].set_title('Learning Rate Schedule')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Epoch training time\n",
    "    axes[1, 0].plot(epochs_range, metrics.epoch_times, 'purple', marker='o', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    axes[1, 0].set_title('Training Time per Epoch')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Training statistics summary\n",
    "    stats_text = f\"\"\"\n",
    "TRAINING SUMMARY\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Total Epochs: {len(metrics.train_losses)}\n",
    "Best Epoch: {metrics.get_best_epoch() + 1}\n",
    "Best Val Loss: {min(metrics.val_losses):.4f}\n",
    "Final Train Loss: {metrics.train_losses[-1]:.4f}\n",
    "Final Val Loss: {metrics.val_losses[-1]:.4f}\n",
    "\n",
    "TRAINING TIME\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Total Time: {sum(metrics.epoch_times):.1f}s\n",
    "Avg Time/Epoch: {np.mean(metrics.epoch_times):.1f}s\n",
    "Min Time/Epoch: {min(metrics.epoch_times):.1f}s\n",
    "Max Time/Epoch: {max(metrics.epoch_times):.1f}s\n",
    "\n",
    "MODEL INFO\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Base Model: {BASE_MODEL}\n",
    "Batch Size: {BATCH_SIZE}\n",
    "Learning Rate: {LEARNING_RATE}\n",
    "Device: {device}\n",
    "Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n",
    "\"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].set_title('Training Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    training_plot_path = os.path.join(output_dir, 'plots', 'training_analysis.png')\n",
    "    plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Training analysis plot saved: {training_plot_path}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No training metrics available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model evaluation and performance testing\n",
    "print(f\"üîç Evaluating fine-tuned model performance...\")\n",
    "\n",
    "if os.path.exists(os.path.join(output_dir, 'best_model.pth')):\n",
    "    # Load best model\n",
    "    print(f\"üìñ Loading best model for evaluation...\")\n",
    "    best_model_path = os.path.join(output_dir, 'best_model.pth')\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    # Create evaluation model\n",
    "    eval_model = sbm.PhaseNet.from_pretrained(BASE_MODEL)\n",
    "    eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    eval_model = eval_model.to(device)\n",
    "    eval_model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Best model loaded (Epoch {checkpoint['epoch'] + 1}, Loss: {checkpoint['loss']:.4f})\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"üß™ Running evaluation on validation set...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            waveforms = batch['waveform'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            metadata = batch['metadata']\n",
    "            \n",
    "            predictions = eval_model(waveforms)\n",
    "            predictions = torch.sigmoid(predictions)  # Apply sigmoid for probabilities\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            if batch_idx == 0:  # Show first batch for visualization\n",
    "                # Create sample prediction visualization\n",
    "                fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "                fig.suptitle('Sample Model Predictions vs Ground Truth', fontsize=14, fontweight='bold')\n",
    "                \n",
    "                sample_idx = 0\n",
    "                sample_waveform = waveforms[sample_idx].cpu().numpy()\n",
    "                sample_pred = predictions[sample_idx].cpu().numpy()\n",
    "                sample_target = labels[sample_idx].cpu().numpy()\n",
    "                \n",
    "                time_axis = np.arange(sample_waveform.shape[1]) / 100  # 100 Hz sampling\n",
    "                \n",
    "                # Plot waveform components\n",
    "                components = ['Z', 'N', 'E']\n",
    "                for i, comp in enumerate(components):\n",
    "                    axes[i, 0].plot(time_axis, sample_waveform[i], 'k-', linewidth=1, alpha=0.7)\n",
    "                    axes[i, 0].set_ylabel(f'{comp} Component')\n",
    "                    axes[i, 0].set_title(f'Waveform - {comp} Component')\n",
    "                    axes[i, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Plot predictions vs targets\n",
    "                phase_labels = ['P-wave', 'S-wave', 'Noise']\n",
    "                colors = ['blue', 'red', 'gray']\n",
    "                \n",
    "                for i, (phase, color) in enumerate(zip(phase_labels, colors)):\n",
    "                    ax = axes[i, 1]\n",
    "                    ax.plot(time_axis, sample_target[i], color=color, linewidth=2, alpha=0.7, label='Ground Truth')\n",
    "                    ax.plot(time_axis, sample_pred[i], color=color, linewidth=2, linestyle='--', label='Prediction')\n",
    "                    ax.set_ylabel('Probability')\n",
    "                    ax.set_title(f'{phase} Probability')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    ax.set_ylim(0, 1)\n",
    "                \n",
    "                axes[-1, 0].set_xlabel('Time (seconds)')\n",
    "                axes[-1, 1].set_xlabel('Time (seconds)')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                sample_pred_path = os.path.join(output_dir, 'plots', 'sample_predictions.png')\n",
    "                plt.savefig(sample_pred_path, dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"üíæ Sample predictions plot saved: {sample_pred_path}\")\n",
    "    \n",
    "    # Concatenate all results\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    print(f\"‚úÖ Evaluation completed\")\n",
    "    print(f\"   Evaluated samples: {all_predictions.shape[0]}\")\n",
    "    print(f\"   Prediction shape: {all_predictions.shape}\")\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "    \n",
    "    # Threshold predictions\n",
    "    threshold = 0.5\n",
    "    pred_binary = (all_predictions > threshold).astype(int)\n",
    "    target_binary = (all_targets > threshold).astype(int)\n",
    "    \n",
    "    performance_metrics = {}\n",
    "    phase_names = ['P-wave', 'S-wave', 'Noise']\n",
    "    \n",
    "    for i, phase in enumerate(phase_names):\n",
    "        # Flatten for phase-specific metrics\n",
    "        y_true = target_binary[:, i, :].flatten()\n",
    "        y_pred = pred_binary[:, i, :].flatten()\n",
    "        y_prob = all_predictions[:, i, :].flatten()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='binary', zero_division=0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_prob)\n",
    "        except ValueError:\n",
    "            auc = 0.0  # Handle case where all samples are one class\n",
    "        \n",
    "        performance_metrics[phase] = {\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1_score': float(f1),\n",
    "            'auc': float(auc)\n",
    "        }\n",
    "    \n",
    "    # Save evaluation results\n",
    "    evaluation_results = {\n",
    "        'model_info': {\n",
    "            'base_model': BASE_MODEL,\n",
    "            'best_epoch': checkpoint['epoch'] + 1,\n",
    "            'best_loss': float(checkpoint['loss']),\n",
    "            'total_parameters': sum(p.numel() for p in eval_model.parameters()),\n",
    "            'trainable_parameters': sum(p.numel() for p in eval_model.parameters() if p.requires_grad)\n",
    "        },\n",
    "        'evaluation_metrics': performance_metrics,\n",
    "        'evaluation_samples': int(all_predictions.shape[0]),\n",
    "        'threshold_used': threshold,\n",
    "        'evaluation_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    eval_results_path = os.path.join(output_dir, 'evaluation_results.json')\n",
    "    with open(eval_results_path, 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Evaluation results saved: {eval_results_path}\")\n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    for phase, metrics in performance_metrics.items():\n",
    "        print(f\"   {phase}:\")\n",
    "        print(f\"     Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"     Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"     F1-Score: {metrics['f1_score']:.3f}\")\n",
    "        print(f\"     AUC: {metrics['auc']:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No best model found for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finalize and create comprehensive summary\n",
    "print(f\"üìã Finalizing fine-tuning process...\")\n",
    "\n",
    "# Create final summary report\n",
    "final_summary = {\n",
    "    \"notebook_info\": {\n",
    "        \"name\": \"Fine_Tune_PhaseNet_Japan\",\n",
    "        \"version\": \"2.0\",\n",
    "        \"description\": \"PhaseNet fine-tuning for Japanese earthquake data using SeisBench\",\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": datetime.now().isoformat(),\n",
    "        \"total_runtime_seconds\": (datetime.now() - start_time).total_seconds()\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"validation_split\": VALIDATION_SPLIT,\n",
    "        \"device\": str(device),\n",
    "        \"early_stopping_patience\": early_stopping_patience\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(full_dataset),\n",
    "        \"training_samples\": train_size,\n",
    "        \"validation_samples\": val_size,\n",
    "        \"synthetic_data_used\": True  # Since we used synthetic data for demo\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"epochs_completed\": len(metrics.train_losses),\n",
    "        \"best_epoch\": metrics.get_best_epoch() + 1 if len(metrics.train_losses) > 0 else 0,\n",
    "        \"best_validation_loss\": min(metrics.val_losses) if len(metrics.val_losses) > 0 else None,\n",
    "        \"final_training_loss\": metrics.train_losses[-1] if len(metrics.train_losses) > 0 else None,\n",
    "        \"total_training_time_seconds\": sum(metrics.epoch_times) if len(metrics.epoch_times) > 0 else 0,\n",
    "        \"average_epoch_time_seconds\": np.mean(metrics.epoch_times) if len(metrics.epoch_times) > 0 else 0\n",
    "    },\n",
    "    \"output_files\": {\n",
    "        \"best_model\": f\"{output_dir}/best_model.pth\",\n",
    "        \"training_metrics\": f\"{output_dir}/metrics/training_metrics.json\",\n",
    "        \"evaluation_results\": f\"{output_dir}/evaluation_results.json\",\n",
    "        \"training_plots\": f\"{output_dir}/plots/training_analysis.png\",\n",
    "        \"sample_predictions\": f\"{output_dir}/plots/sample_predictions.png\"\n",
    "    },\n",
    "    \"success\": len(metrics.train_losses) > 0 and os.path.exists(os.path.join(output_dir, 'best_model.pth'))\n",
    "}\n",
    "\n",
    "# Save final summary\n",
    "final_summary_path = os.path.join(output_dir, 'final_summary.json')\n",
    "with open(final_summary_path, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Final summary saved: {final_summary_path}\")\n",
    "\n",
    "# Call notebook finalization\n",
    "output_files = [\n",
    "    f\"{output_dir}/best_model.pth\",\n",
    "    f\"{output_dir}/final_summary.json\",\n",
    "    f\"{output_dir}/metrics/training_metrics.json\"\n",
    "]\n",
    "\n",
    "# Add optional files if they exist\n",
    "optional_files = [\n",
    "    f\"{output_dir}/evaluation_results.json\",\n",
    "    f\"{output_dir}/plots/training_analysis.png\",\n",
    "    f\"{output_dir}/plots/sample_predictions.png\"\n",
    "]\n",
    "\n",
    "for file_path in optional_files:\n",
    "    if os.path.exists(file_path):\n",
    "        output_files.append(file_path)\n",
    "\n",
    "notebook_finalize(\n",
    "    config=config,\n",
    "    success=final_summary['success'],\n",
    "    output_files=output_files,\n",
    "    summary=final_summary\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ PhaseNet Fine-Tuning Complete!\")\n",
    "print(f\"   Runtime: {(datetime.now() - start_time).total_seconds():.1f} seconds\")\n",
    "print(f\"   Success: {'‚úÖ Yes' if final_summary['success'] else '‚ùå No'}\")\n",
    "print(f\"   Output directory: {output_dir}\")\n",
    "\n",
    "if final_summary['success']:\n",
    "    print(f\"\\nüìà Training Summary:\")\n",
    "    print(f\"   Epochs completed: {final_summary['training_results']['epochs_completed']}\")\n",
    "    print(f\"   Best epoch: {final_summary['training_results']['best_epoch']}\")\n",
    "    print(f\"   Best validation loss: {final_summary['training_results']['best_validation_loss']:.4f}\")\n",
    "    print(f\"   Average epoch time: {final_summary['training_results']['average_epoch_time_seconds']:.1f}s\")\n",
    "    print(f\"   Fine-tuned model ready for Japanese earthquake phase picking!\")\n",
    "else:\n",
    "    print(f\"\\nüîß Training did not complete successfully - check logs for issues\")\n",
    "\n",
    "print(f\"\\nüìö Next Steps:\")\n",
    "print(f\"   1. Test fine-tuned model on real Japanese earthquake data\")\n",
    "print(f\"   2. Compare performance with original PhaseNet model\")\n",
    "print(f\"   3. Deploy model for production earthquake monitoring\")\n",
    "print(f\"   4. Continue fine-tuning with additional Japanese datasets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",\n",
   "language": "python",\n",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}