{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Detection Workflow\n",
    "\n",
    "## Outline\n",
    "\n",
    "Here we show an example of the current modules in QuakeFlow\n",
    "\n",
    "1. Download data using Obpsy:\n",
    "\n",
    "    [FDSN web service client for ObsPy](https://docs.obspy.org/packages/obspy.clients.fdsn.html#module-obspy.clients.fdsn)\n",
    "    \n",
    "    [Mass Downloader for FDSN Compliant Web Services](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.mass_downloader.html#module-obspy.clients.fdsn.mass_downloader)\n",
    "\n",
    "2. PhaseNet for picking P/S phases\n",
    "\n",
    "    Find more details in [PhaseNet github page](https://wayneweiqiang.github.io/PhaseNet/)\n",
    "\n",
    "3. GaMMA for associating picking and estimate approximate location and magnitude\n",
    "\n",
    "    Find more details in [GaMMA github page](https://wayneweiqiang.github.io/GMMA/)\n",
    "\n",
    "4. Earthquake location, magnitude estimation, etc. (to be continued)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install [miniconda](https://docs.conda.io/en/latest/miniconda.html) and download packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T07:24:29.845680Z",
     "iopub.status.busy": "2021-07-23T07:24:29.845272Z",
     "iopub.status.idle": "2021-07-23T07:24:29.922435Z",
     "shell.execute_reply": "2021-07-23T07:24:29.917867Z",
     "shell.execute_reply.started": "2021-07-23T07:24:29.845649Z"
    },
    "tags": []
   },
   "source": [
    "<!-- # %%capture -->\n",
    "```bash\n",
    "git clone https://github.com/wayneweiqiang/PhaseNet.git\n",
    "git clone https://github.com/wayneweiqiang/GMMA.git\n",
    "conda env update -f=env.yml -n base\n",
    "```\n",
    "\n",
    "**Second option: install to quakeflow environment, but need to select jupyter notebook kernel to quakflow**\n",
    "```bash\n",
    "conda env create -f=env.yml -n quakeflow\n",
    "python -m ipykernel install --user --name=quakeflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "# matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# region_name = \"Ridgecrest_demo\"\n",
    "# region_name = \"Ridgecrest_oneweek\"\n",
    "# region_name = \"SaltonSea\"\n",
    "# region_name = \"Ridgecrest\"\n",
    "# region_name = \"SanSimeon\"\n",
    "# region_name = \"Italy\"\n",
    "# region_name = \"PNSN\"\n",
    "region_name = \"Hawaii\"\n",
    "# region_name = \"PuertoRico\"\n",
    "# region_name = \"SmithValley\"\n",
    "# region_name = \"Antilles\"\n",
    "dir_name = region_name\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "root_dir = lambda x: os.path.join(dir_name, x)\n",
    "\n",
    "run_local = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(\n",
    "    index_json: OutputPath(\"json\"),\n",
    "    config_json: OutputPath(\"json\"),\n",
    "    datetime_json: OutputPath(\"json\"),\n",
    "    num_parallel: int = 1,\n",
    ") -> list:\n",
    "\n",
    "    import obspy\n",
    "    import datetime\n",
    "    import json\n",
    "\n",
    "    pi = 3.1415926\n",
    "    degree2km = pi * 6371 / 180\n",
    "\n",
    "    # region_name = \"Ridgecrest_demo\"\n",
    "    # center = (-117.504, 35.705)\n",
    "    # horizontal_degree = 1.0\n",
    "    # vertical_degree = 1.0\n",
    "    # starttime = obspy.UTCDateTime(\"2019-07-04T17\")\n",
    "    # endtime = obspy.UTCDateTime(\"2019-07-04T19\")\n",
    "    # client = \"SCEDC\"\n",
    "    # network_list = [\"CI\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    # region_name = \"Ridgecrest_oneweek\"\n",
    "    # center = (-117.504, 35.705)\n",
    "    # horizontal_degree = 1.0\n",
    "    # vertical_degree = 1.0\n",
    "    # starttime = obspy.UTCDateTime(\"2019-07-04T00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2019-07-10T00\")\n",
    "    # client = \"SCEDC\"\n",
    "    # network_list = [\"CI\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    region_name = \"Hawaii\"\n",
    "    center = (-155.32, 19.39)\n",
    "    horizontal_degree = 2.0\n",
    "    vertical_degree = 2.0\n",
    "    starttime = obspy.UTCDateTime(\"2021-07-01T00\")\n",
    "    endtime = obspy.UTCDateTime(\"2021-11-01T00\")\n",
    "    client = \"IRIS\"\n",
    "    network_list = [\"HV\", \"PT\"]\n",
    "    channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    # region_name = \"PuertoRico\"\n",
    "    # center = (-66.5, 18)\n",
    "    # horizontal_degree = 3.0\n",
    "    # vertical_degree = 2.0\n",
    "    # # starttime = obspy.UTCDateTime(\"2020-01-01T00\")\n",
    "    # # endtime = obspy.UTCDateTime(\"2020-01-05T05\")\n",
    "    # starttime = obspy.UTCDateTime(\"2018-01-01T00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2018-01-01T06\")\n",
    "    # # endtime = obspy.UTCDateTime(\"2018-01-06T00\")\n",
    "    # # endtime = obspy.UTCDateTime(\"2021-08-01T00\")\n",
    "    # client = \"IRIS\"\n",
    "    # network_list = [\"*\"]\n",
    "    # channel_list = \"HH*,BH*,HN*\"\n",
    "\n",
    "    # region_name = \"SaltonSea\"\n",
    "    # center = (-115.53, 32.98)\n",
    "    # horizontal_degree = 1.0\n",
    "    # vertical_degree = 1.0\n",
    "    # starttime = obspy.UTCDateTime(\"2020-10-01T00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2020-10-01T02\")\n",
    "    # client = \"SCEDC\"\n",
    "    # network_list = [\"CI\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    # region_name = \"2003SanSimeon\"\n",
    "    # center = (-121.101, 35.701)\n",
    "    # horizontal_degree = 1.0\n",
    "    # vertical_degree = 1.0\n",
    "    # starttime = obspy.UTCDateTime(\"2003-12-22T00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2003-12-24T00\")\n",
    "    # client = \"NCEDC\"\n",
    "    # network_list = [\"*\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    # region_name = \"Italy\"\n",
    "    # center = (13.188, 42.723)\n",
    "    # horizontal_degree = 1.0\n",
    "    # vertical_degree = 1.0\n",
    "    # starttime = obspy.UTCDateTime(\"2016-08-24T00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2016-08-26T00\")\n",
    "    # client = \"INGV\"\n",
    "    # network_list = [\"*\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    # region_name = \"SmithValley\"\n",
    "    # center = (-119.5, 38.51)\n",
    "    # horizontal_degree = 1.0\n",
    "    # vertical_degree = 1.0\n",
    "    # starttime = obspy.UTCDateTime(\"2021-07-08T00:00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2021-07-16T00:00\")\n",
    "    # client = \"NCEDC\"\n",
    "    # network_list = [\"*\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    # region_name = \"Antilles\"\n",
    "    # center = (-61.14867, 14.79683)\n",
    "    # horizontal_degree = 0.2\n",
    "    # vertical_degree = 0.2\n",
    "    # starttime = obspy.UTCDateTime(\"2021-04-10T00\")\n",
    "    # endtime = obspy.UTCDateTime(\"2021-04-15T00\")\n",
    "    # client = \"RESIF\"\n",
    "    # network_list = [\"*\"]\n",
    "    # channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    ####### save config ########\n",
    "    config = {}\n",
    "    config[\"region\"] = region_name\n",
    "    config[\"center\"] = center\n",
    "    config[\"xlim_degree\"] = [center[0] - horizontal_degree / 2, center[0] + horizontal_degree / 2]\n",
    "    config[\"ylim_degree\"] = [center[1] - vertical_degree / 2, center[1] + vertical_degree / 2]\n",
    "    config[\"degree2km\"] = degree2km\n",
    "    config[\"starttime\"] = starttime.datetime.isoformat()\n",
    "    config[\"endtime\"] = endtime.datetime.isoformat()\n",
    "    config[\"networks\"] = network_list\n",
    "    config[\"channels\"] = channel_list\n",
    "    config[\"client\"] = client\n",
    "\n",
    "    one_day = datetime.timedelta(days=1)\n",
    "    one_hour = datetime.timedelta(hours=1)\n",
    "    starttimes = []\n",
    "    tmp_start = starttime\n",
    "    while tmp_start < endtime:\n",
    "        starttimes.append(tmp_start.datetime.isoformat())\n",
    "        tmp_start += one_hour\n",
    "\n",
    "    with open(datetime_json, \"w\") as fp:\n",
    "        json.dump({\"starttimes\": starttimes, \"interval\": one_hour.total_seconds()}, fp)\n",
    "\n",
    "    if num_parallel == 0:\n",
    "        num_parallel = min(24, len(starttimes))\n",
    "        # num_parallel = min(60, len(starttimes)//12)\n",
    "        # num_parallel = (len(starttimes)-1)//(24) + 1\n",
    "        # num_parallel = (len(starttimes)-1)//(24*7) + 1\n",
    "        # num_parallel = (len(starttimes)-1)//(24*10) + 1\n",
    "        # num_parallel = (len(starttimes)-1)//(24*14) + 1\n",
    "        # num_parallel = min(60, (len(starttimes)-1)//(24) + 1)\n",
    "        print(f\"num_parallel = {num_parallel}\")\n",
    "    config[\"num_parallel\"] = num_parallel\n",
    "\n",
    "    idx = [[] for i in range(num_parallel)]\n",
    "    for i in range(len(starttimes)):\n",
    "        idx[i - i // num_parallel * num_parallel].append(i)\n",
    "\n",
    "    with open(config_json, 'w') as fp:\n",
    "        json.dump(config, fp)\n",
    "    with open(index_json, 'w') as fp:\n",
    "        json.dump(idx, fp)\n",
    "\n",
    "    return list(range(num_parallel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    idx = set_config(root_dir(\"index.json\"), root_dir(\"config.json\"), root_dir(\"datetimes.json\"), num_parallel=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_op = comp.func_to_container_op(\n",
    "    set_config,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\n",
    "        \"obspy\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download events in the routine catalog\n",
    "\n",
    "This catalog is not used by QuakeFolow. It is only used for comparing detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_events(config_json: InputPath(\"json\"), event_csv: OutputPath(str)):\n",
    "\n",
    "    from obspy.clients.fdsn import Client\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import matplotlib\n",
    "    #     matplotlib.use(\"agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    ####### IRIS catalog ########\n",
    "    try:\n",
    "        events = Client(config[\"client\"]).get_events(\n",
    "            starttime=config[\"starttime\"],\n",
    "            endtime=config[\"endtime\"],\n",
    "            minlongitude=config[\"xlim_degree\"][0],\n",
    "            maxlongitude=config[\"xlim_degree\"][1],\n",
    "            minlatitude=config[\"ylim_degree\"][0],\n",
    "            maxlatitude=config[\"ylim_degree\"][1],\n",
    "        )\n",
    "    except:\n",
    "        events = Client(\"iris\").get_events(\n",
    "            starttime=config[\"starttime\"],\n",
    "            endtime=config[\"endtime\"],\n",
    "            minlongitude=config[\"xlim_degree\"][0],\n",
    "            maxlongitude=config[\"xlim_degree\"][1],\n",
    "            minlatitude=config[\"ylim_degree\"][0],\n",
    "            maxlatitude=config[\"ylim_degree\"][1],\n",
    "        )\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "\n",
    "    ####### Save catalog ########\n",
    "    catalog = defaultdict(list)\n",
    "    for event in events:\n",
    "        if len(event.magnitudes) > 0:\n",
    "            catalog[\"time\"].append(event.origins[0].time.datetime)\n",
    "            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n",
    "            catalog[\"longitude\"].append(event.origins[0].longitude)\n",
    "            catalog[\"latitude\"].append(event.origins[0].latitude)\n",
    "            catalog[\"depth(m)\"].append(event.origins[0].depth)\n",
    "    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n",
    "    catalog.to_csv(\n",
    "        event_csv,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        float_format=\"%.3f\",\n",
    "        date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "        columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\"],\n",
    "    )\n",
    "\n",
    "    ####### Plot catalog ########\n",
    "    plt.figure()\n",
    "    plt.plot(catalog[\"longitude\"], catalog[\"latitude\"], '.', markersize=1)\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.xlim(config[\"xlim_degree\"])\n",
    "    plt.ylim(config[\"ylim_degree\"])\n",
    "    #  plt.savefig(os.path.join(data_path, \"events_loc.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot_date(catalog[\"time\"], catalog[\"magnitude\"], '.', markersize=1)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.title(f\"Number of events: {len(events)}\")\n",
    "    #  plt.savefig(os.path.join(data_path, \"events_mag_time.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    download_events(root_dir(\"config.json\"), root_dir(\"events.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_events_op = comp.func_to_container_op(\n",
    "    download_events,\n",
    "    #base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\n",
    "        \"obspy\",\n",
    "        \"pandas\",\n",
    "        \"matplotlib\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stations(config_json: InputPath(\"json\"), station_csv: OutputPath(str), station_pkl: OutputPath(\"pickle\")):\n",
    "\n",
    "    import pickle\n",
    "    from obspy.clients.fdsn import Client\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import matplotlib\n",
    "    #     matplotlib.use(\"agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    print(\"Network:\", \",\".join(config[\"networks\"]))\n",
    "    ####### Download stations ########\n",
    "    stations = Client(config[\"client\"]).get_stations(\n",
    "        network=\",\".join(config[\"networks\"]),\n",
    "        station=\"*\",\n",
    "        starttime=config[\"starttime\"],\n",
    "        endtime=config[\"endtime\"],\n",
    "        minlongitude=config[\"xlim_degree\"][0],\n",
    "        maxlongitude=config[\"xlim_degree\"][1],\n",
    "        minlatitude=config[\"ylim_degree\"][0],\n",
    "        maxlatitude=config[\"ylim_degree\"][1],\n",
    "        channel=config[\"channels\"],\n",
    "        level=\"response\",\n",
    "    ) \n",
    "    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n",
    "\n",
    "    ####### Save stations ########\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            for chn in station:\n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\"\n",
    "                if sid in station_locs:\n",
    "                    station_locs[sid][\"component\"] += f\",{chn.code[-1]}\"\n",
    "                    station_locs[sid][\"response\"] += f\",{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                else:\n",
    "                    component = f\"{chn.code[-1]}\"\n",
    "                    response = f\"{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                    dtype = chn.response.instrument_sensitivity.input_units.lower()\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict[\"longitude\"], tmp_dict[\"latitude\"], tmp_dict[\"elevation(m)\"] = (\n",
    "                        chn.longitude,\n",
    "                        chn.latitude,\n",
    "                        chn.elevation,\n",
    "                    )\n",
    "                    tmp_dict[\"component\"], tmp_dict[\"response\"], tmp_dict[\"unit\"] = component, response, dtype\n",
    "                    station_locs[sid] = tmp_dict\n",
    "\n",
    "    station_locs = pd.DataFrame.from_dict(station_locs, orient='index')\n",
    "    station_locs.to_csv(\n",
    "        station_csv,\n",
    "        sep=\"\\t\",\n",
    "        float_format=\"%.3f\",\n",
    "        index_label=\"station\",\n",
    "        columns=[\"longitude\", \"latitude\", \"elevation(m)\", \"unit\", \"component\", \"response\"],\n",
    "    )\n",
    "\n",
    "    with open(station_pkl, \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)\n",
    "\n",
    "    ######## Plot stations ########\n",
    "    plt.figure()\n",
    "    plt.plot(station_locs[\"longitude\"], station_locs[\"latitude\"], \"^\", label=\"Stations\")\n",
    "    plt.xlabel(\"X (km)\")\n",
    "    plt.ylabel(\"Y (km)\")\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.xlim(config[\"xlim_degree\"])\n",
    "    plt.ylim(config[\"ylim_degree\"])\n",
    "    plt.legend()\n",
    "    plt.title(f\"Number of stations: {len(station_locs)}\")\n",
    "    #     plt.savefig(os.path.join(data_path, \"stations_loc.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    download_stations(root_dir(\"config.json\"), root_dir(\"stations.csv\"), root_dir(\"stations.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_stations_op = comp.func_to_container_op(\n",
    "    download_stations,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\n",
    "        \"obspy\",\n",
    "        \"pandas\",\n",
    "        \"matplotlib\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download waveform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_waveform(\n",
    "    i: int,\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    datetime_json: InputPath(\"json\"),\n",
    "    station_pkl: InputPath(\"pickle\"),\n",
    "    fname_csv: OutputPath(str),\n",
    "    data_path: str,\n",
    "    bucket_name: str = \"waveforms\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    ") -> str:\n",
    "\n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    import time\n",
    "    import json\n",
    "    import random\n",
    "    import threading\n",
    "\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    upload_minio = False\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key='minio', secret_key='minio123', secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "        upload_minio = True\n",
    "    except Exception as err:\n",
    "        # print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[i]\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    with open(datetime_json, \"r\") as fp:\n",
    "        tmp = json.load(fp)\n",
    "        starttimes = tmp[\"starttimes\"]\n",
    "        interval = tmp[\"interval\"]\n",
    "    with open(station_pkl, \"rb\") as fp:\n",
    "        stations = pickle.load(fp)\n",
    "\n",
    "    waveform_dir = os.path.join(data_path, config[\"region\"], \"waveforms\")\n",
    "    if not os.path.exists(waveform_dir):\n",
    "        os.makedirs(waveform_dir)\n",
    "\n",
    "    ####### Download data ########\n",
    "    client = Client(config[\"client\"])\n",
    "    fname_list = [\"fname\"]\n",
    "\n",
    "    def download(i):\n",
    "        #     for i in idx:\n",
    "        starttime = obspy.UTCDateTime(starttimes[i])\n",
    "        endtime = starttime + interval\n",
    "        fname = \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "        if not upload_minio:\n",
    "            if os.path.exists(os.path.join(waveform_dir, fname)):\n",
    "                print(f\"{fname} exists\")\n",
    "                fname_list.append(fname)\n",
    "                return\n",
    "        else:\n",
    "            try:\n",
    "                minioClient.fget_object(bucket_name,  os.path.join(config['region'], fname), os.path.join(waveform_dir, fname))\n",
    "                print(f\"{bucket_name}/{os.path.join(config['region'], fname)} download to {os.path.join(waveform_dir, fname)}\")\n",
    "                fname_list.append(fname)\n",
    "                return\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "\n",
    "        max_retry = 10\n",
    "        stream = obspy.Stream()\n",
    "        print(f\"{fname} download starts\")\n",
    "        num_sta = 0\n",
    "        for network in stations:\n",
    "            for station in network:\n",
    "                print(f\"********{network.code}.{station.code}********\")\n",
    "                retry = 0\n",
    "                while retry < max_retry:\n",
    "                    try:\n",
    "                        tmp = client.get_waveforms(\n",
    "                            network.code, station.code, \"*\", config[\"channels\"], starttime, endtime\n",
    "                        )\n",
    "                        #  for trace in tmp:\n",
    "                        #      if trace.stats.sampling_rate != 100:\n",
    "                        #          print(trace)\n",
    "                        #          trace = trace.interpolate(100, method=\"linear\")\n",
    "                        #      trace = trace.detrend(\"spline\", order=2, dspline=5*trace.stats.sampling_rate)\n",
    "                        #      stream.append(trace)\n",
    "                        stream += tmp\n",
    "                        num_sta += len(tmp)\n",
    "                        break\n",
    "                    except Exception as err:\n",
    "                        print(\"Error {}.{}: {}\".format(network.code, station.code, err))\n",
    "                        message = \"No data available for request.\"\n",
    "                        if str(err)[: len(message)] == message:\n",
    "                            break\n",
    "                        retry += 1\n",
    "                        time.sleep(5)\n",
    "                        continue\n",
    "                if retry == max_retry:\n",
    "                    print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code}\")\n",
    "\n",
    "        # stream = stream.merge(fill_value=0)\n",
    "        # stream = stream.trim(starttime, endtime, pad=True, fill_value=0)\n",
    "        stream.write(os.path.join(waveform_dir, fname))\n",
    "        print(f\"{fname} download succeeds\")\n",
    "        if upload_minio:\n",
    "            minioClient.fput_object(bucket_name, os.path.join(config['region'], fname), os.path.join(waveform_dir, fname))\n",
    "            print(f\"{fname} upload to minio {os.path.join(config['region'], fname)}\")\n",
    "        lock.acquire()\n",
    "        fname_list.append(fname)\n",
    "        lock.release()\n",
    "\n",
    "    threads = []\n",
    "    MAX_THREADS = 4\n",
    "    for ii, i in enumerate(idx):\n",
    "        t = threading.Thread(target=download, args=(i,))\n",
    "        t.start()\n",
    "        time.sleep(1)\n",
    "        threads.append(t)\n",
    "        if ii % MAX_THREADS == MAX_THREADS - 1:\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            threads = []\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    with open(fname_csv, \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(fname_list))\n",
    "\n",
    "    return waveform_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    waveform_path = download_waveform(\n",
    "        0,\n",
    "        root_dir(\"index.json\"),\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"datetimes.json\"),\n",
    "        root_dir(\"stations.pkl\"),\n",
    "        root_dir(\"fname.csv\"),\n",
    "        data_path=root_dir(\"\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_waveform_op = comp.func_to_container_op(\n",
    "    download_waveform,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\"obspy\", \"minio\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phasenet2gamma(\n",
    "    i: int,\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    station_csv: InputPath(str),\n",
    "    data_path: str,\n",
    "    data_list: InputPath(str),\n",
    "    catalog_csv: OutputPath(str),\n",
    "    picks_csv: OutputPath(str),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"localhost:9000\",\n",
    "    secure: bool = True,\n",
    ") -> str:\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    import obspy\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    import multiprocessing\n",
    "    from multiprocessing import dummy\n",
    "    import time\n",
    "\n",
    "    import threading\n",
    "\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    # from requests.adapters import HTTPAdapter\n",
    "    # from requests.packages.urllib3.util.retry import Retry\n",
    "    # retry_strategy = Retry(\n",
    "    #     total=10,\n",
    "    #     status_forcelist=[104, 502],\n",
    "    # )\n",
    "    # adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    # http = requests.Session()\n",
    "    # http.mount(\"http://\", adapter)\n",
    "\n",
    "    def convert_mseed(\n",
    "        mseed, stations, sampling_rate=100, n_channel=3, dtype=\"float32\", amplitude=True, remove_resp=True\n",
    "    ):\n",
    "        try:\n",
    "            mseed = mseed.detrend(\"spline\", order=2, dspline=5 * mseed[0].stats.sampling_rate)\n",
    "        except:\n",
    "            print(f\"Error: spline detrend failed at file {mseed}\")\n",
    "            mseed = mseed.detrend(\"demean\")\n",
    "        mseed = mseed.merge(fill_value=0)\n",
    "        starttime = min([st.stats.starttime for st in mseed])\n",
    "        endtime = max([st.stats.endtime for st in mseed])\n",
    "        # endtime = starttime + 60\n",
    "        mseed = mseed.trim(starttime, endtime, pad=True, fill_value=0)\n",
    "\n",
    "        for i in range(len(mseed)):\n",
    "            if mseed[i].stats.sampling_rate != sampling_rate:\n",
    "                # print(f\"Resampling {mseed[i].id} from {mseed[i].stats.sampling_rate} to {sampling_rate} Hz\")\n",
    "                mseed[i] = mseed[i].interpolate(sampling_rate, method=\"linear\")\n",
    "\n",
    "        order = ['3', '2', '1', 'E', 'N', 'Z']\n",
    "        order = {key: i for i, key in enumerate(order)}\n",
    "        comp2idx = {\"3\": 0, \"2\": 1, \"1\": 2, \"E\": 0, \"N\": 1, \"Z\": 2}\n",
    "\n",
    "        nsta = len(stations)\n",
    "        nt = max(len(mseed[i].data) for i in range(len(mseed)))\n",
    "        data = []\n",
    "        station_id = []\n",
    "        t0 = []\n",
    "        for i in range(nsta):\n",
    "            trace_data = np.zeros([nt, n_channel], dtype=dtype)\n",
    "            empty_station = True\n",
    "            # sta = stations.iloc[i][\"station\"]\n",
    "            # sta = stations.index[i]\n",
    "            sta = stations.iloc[i][\"id\"]\n",
    "            comp = stations.iloc[i][\"component\"].split(\",\")\n",
    "            if remove_resp:\n",
    "                resp = stations.iloc[i][\"response\"].split(\",\")\n",
    "                # resp = station_locs.iloc[i][\"response\"]\n",
    "\n",
    "            for j, c in enumerate(sorted(comp, key=lambda x: order[x[-1]])):\n",
    "                resp_j = float(resp[j])\n",
    "                if len(comp) != 3:  ## less than 3 component\n",
    "                    j = comp2idx[c]\n",
    "                if len(mseed.select(id=sta + c)) == 0:\n",
    "                    # print(f\"Empty trace: {sta+c} {starttime}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    empty_station = False\n",
    "\n",
    "                tmp = mseed.select(id=sta + c)[0].data.astype(dtype)\n",
    "                trace_data[: len(tmp), j] = tmp[:nt]\n",
    "\n",
    "                if stations.iloc[i][\"unit\"] == \"m/s**2\":\n",
    "                    tmp = mseed.select(id=sta + c)[0]\n",
    "                    tmp = tmp.integrate()\n",
    "                    tmp = tmp.filter(\"highpass\", freq=1.0)\n",
    "                    tmp = tmp.data.astype(dtype)\n",
    "                    trace_data[: len(tmp), j] = tmp[:nt]\n",
    "                elif stations.iloc[i][\"unit\"] == \"m/s\":\n",
    "                    tmp = mseed.select(id=sta + c)[0].data.astype(dtype)\n",
    "                    trace_data[: len(tmp), j] = tmp[:nt]\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Error in {stations.iloc[i]['station']}\\n{stations.iloc[i]['unit']} should be m/s**2 or m/s!\"\n",
    "                    )\n",
    "\n",
    "                if remove_resp:\n",
    "                    trace_data[:, j] /= resp_j\n",
    "\n",
    "            if not empty_station:\n",
    "                data.append(trace_data)\n",
    "                station_id.append(sta)\n",
    "                t0.append(starttime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3])\n",
    "\n",
    "        data = np.stack(data)\n",
    "\n",
    "        meta = {\"data\": data, \"t0\": t0, \"station_id\": station_id}\n",
    "\n",
    "        return meta\n",
    "\n",
    "    # PHASENET_API_URL = \"http://phasenet.quakeflow.com\"\n",
    "    # GAMMA_API_URL = \"http://gamma.quakeflow.com\"\n",
    "    PHASENET_API_URL = \"http://phasenet-api.default.svc.cluster.local:8000\"\n",
    "    GAMMA_API_URL = \"http://gamma-api.default.svc.cluster.local:8001\"\n",
    "\n",
    "    ## read config\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[i]\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    ## read stations\n",
    "    stations = pd.read_csv(station_csv, delimiter=\"\\t\")\n",
    "    stations = stations.rename(columns={\"station\": \"id\"})\n",
    "\n",
    "    ## read meed list\n",
    "    data_list = pd.read_csv(data_list)\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    # catalog_list = manager.list()\n",
    "    # picks_list = manager.list()\n",
    "    catalog_list = []\n",
    "    picks_list = []\n",
    "\n",
    "    for fname in data_list['fname']:\n",
    "        # def process(fname):\n",
    "        # print(f\"Process {fname}\\n\")\n",
    "        mseed = obspy.read(os.path.join(data_path, fname))\n",
    "        meta = convert_mseed(mseed, stations)\n",
    "\n",
    "        batch = 2\n",
    "        # phasenet_picks = manager.list()\n",
    "        phasenet_picks = []\n",
    "\n",
    "        # for j in range(0, len(meta[\"station_id\"]), batch):\n",
    "        def run_phasenet(j):\n",
    "            req = {\n",
    "                \"id\": meta['station_id'][j : j + batch],\n",
    "                \"timestamp\": meta[\"t0\"][j : j + batch],\n",
    "                \"vec\": meta[\"data\"][j : j + batch].tolist(),\n",
    "            }\n",
    "            # resp = requests.post(f'{PHASENET_API_URL}/predict', json=req)\n",
    "            # phasenet_picks.extend(resp.json())\n",
    "            counts = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    resp = requests.post(f'{PHASENET_API_URL}/predict', json=req, timeout=30)\n",
    "                    lock.acquire()\n",
    "                    phasenet_picks.extend(resp.json())\n",
    "                    lock.release()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if counts > 30:\n",
    "                        print(f\"Error PhaseNet on {fname} at {j} trace: {e}\")\n",
    "                        break\n",
    "                    print(f\"Retry PhaseNet on {fname} at {j} trace\")\n",
    "                    # time.sleep(3)\n",
    "                    counts += 1\n",
    "\n",
    "        time_prev = time.time()\n",
    "        print(f\"PhaseNet start on {fname}...\")\n",
    "        threads = []\n",
    "        MAX_THREADS = 4\n",
    "        # for ii, i in enumerate(idx):\n",
    "        for ii, j in enumerate(range(0, len(meta[\"station_id\"]), batch)):\n",
    "            t = threading.Thread(target=run_phasenet, args=(j,))\n",
    "            t.start()\n",
    "            time.sleep(3)\n",
    "            threads.append(t)\n",
    "            if ii % MAX_THREADS == MAX_THREADS - 1:\n",
    "                for t in threads:\n",
    "                    t.join()\n",
    "                threads = []\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        print(f\"PhaseNet finish on {fname} using {time.time()-time_prev}s\")\n",
    "\n",
    "        # print(f\"PhaseNet on {fname}...\")\n",
    "        # # with multiprocessing.dummy.Pool(processes=min(8, (len(meta[\"station_id\"])-1)//batch+1)) as pool:\n",
    "        # with multiprocessing.dummy.Pool(processes=min(8, (len(meta[\"station_id\"])-1)//batch+1)) as pool:\n",
    "        #     pool.map(run_phasenet, range(0, len(meta[\"station_id\"]), batch))\n",
    "        # phasenet_picks = list(phasenet_picks)\n",
    "\n",
    "        time_prev = time.time()\n",
    "        print(f\"GaMMA start on {fname}...\")\n",
    "        counts = 0\n",
    "        while True:\n",
    "            try:\n",
    "                resp = requests.post(\n",
    "                    f'{GAMMA_API_URL}/predict',\n",
    "                    json={\"picks\": phasenet_picks, \"stations\": stations.to_dict(orient=\"records\"), \"config\": config},\n",
    "                )\n",
    "                result = resp.json()\n",
    "                catalog_gamma = result[\"catalog\"]\n",
    "                for c in catalog_gamma:\n",
    "                    c[\"file_idx\"] = fname.split(\"/\")[-1]\n",
    "                picks_gamma = result[\"picks\"]\n",
    "                for c in picks_gamma:\n",
    "                    c[\"file_idx\"] = fname.split(\"/\")[-1]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if counts > 30:\n",
    "                    print(f\"Error in GaMMA on {fname}: {e}\")\n",
    "                    break\n",
    "                print(f\"Retry GaMMA on {fname}\")\n",
    "                time.sleep(3)\n",
    "                counts += 1\n",
    "\n",
    "        catalog_list.extend(catalog_gamma)\n",
    "        picks_list.extend(picks_gamma)\n",
    "\n",
    "        print(f\"GaMMA finish on {fname} using {time.time()-time_prev}s\")\n",
    "\n",
    "    # with multiprocessing.dummy.Pool(processes=min(12, len(data_list[\"fname\"]))) as pool:\n",
    "    #     pool.map(process, data_list[\"fname\"])\n",
    "\n",
    "    catalog_df = pd.DataFrame(list(catalog_list))\n",
    "    picks_df = pd.DataFrame(list(picks_list))\n",
    "    manager.shutdown()\n",
    "    if len(catalog_df) > 0:\n",
    "        print(\"GaMMA catalog:\")\n",
    "        print(\n",
    "            catalog_df[\n",
    "                [\"time\", \"latitude\", \"longitude\", \"depth(m)\", \"magnitude\", \"covariance\", \"event_idx\", \"file_idx\"]\n",
    "            ]\n",
    "        )\n",
    "    if len(picks_df) > 0:\n",
    "        print(\"GaMMA association:\")\n",
    "        print(picks_df)\n",
    "\n",
    "    with open(catalog_csv, 'w') as fp:\n",
    "        catalog_df.to_csv(\n",
    "            fp,\n",
    "            sep=\"\\t\",\n",
    "            index=False,\n",
    "            float_format=\"%.3f\",\n",
    "            date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "            columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\", \"covariance\", \"event_idx\", \"file_idx\"],\n",
    "        )\n",
    "\n",
    "    with open(picks_csv, 'w') as fp:\n",
    "        picks_df.to_csv(\n",
    "            fp,\n",
    "            sep=\"\\t\",\n",
    "            index=False,\n",
    "            date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "            columns=[\"id\", \"timestamp\", \"type\", \"prob\", \"amp\", \"prob_gmma\", \"event_idx\", \"file_idx\"],\n",
    "        )\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        catalog_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "        if not os.path.exists(catalog_dir):\n",
    "            os.makedirs(catalog_dir)\n",
    "        minioClient = Minio(s3_url, access_key='minio', secret_key='minio123', secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        with open(os.path.join(catalog_dir, f\"catalog_{idx[0]:04d}.csv\"), 'w') as fp:\n",
    "            catalog_df.to_csv(\n",
    "                fp,\n",
    "                sep=\"\\t\",\n",
    "                index=False,\n",
    "                float_format=\"%.3f\",\n",
    "                date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                columns=[\n",
    "                    \"time\",\n",
    "                    \"magnitude\",\n",
    "                    \"longitude\",\n",
    "                    \"latitude\",\n",
    "                    \"depth(m)\",\n",
    "                    \"covariance\",\n",
    "                    \"event_idx\",\n",
    "                    \"file_idx\",\n",
    "                ],\n",
    "            )\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/catalog_{idx[0]:04d}.csv\",\n",
    "            os.path.join(catalog_dir, f\"catalog_{idx[0]:04d}.csv\"),\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(catalog_dir, f\"picks_{idx[0]:04d}.csv\"), 'w') as fp:\n",
    "            picks_df.to_csv(\n",
    "                fp,\n",
    "                sep=\"\\t\",\n",
    "                index=False,\n",
    "                date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                columns=[\"id\", \"timestamp\", \"type\", \"prob\", \"amp\", \"prob_gmma\", \"event_idx\", \"file_idx\"],\n",
    "            )\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/picks_{idx[0]:04d}.csv\",\n",
    "            os.path.join(catalog_dir, f\"picks_{idx[0]:04d}.csv\"),\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        # print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    return f\"catalog_{idx[0]:04d}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    phasenet2gamma(\n",
    "        0, \n",
    "        root_dir(\"index.json\"),\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"stations.csv\"),\n",
    "        root_dir(root_dir('waveforms')),\n",
    "        root_dir('fname.csv'),\n",
    "        root_dir(\"catalog.csv\"),\n",
    "        root_dir(\"picks.csv\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phasenet2gamma_op = comp.func_to_container_op(\n",
    "    phasenet2gamma,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"tqdm\", \"minio\", \"obspy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    %run plot_catalog.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Parallel processing on cloud\n",
    "\n",
    "Only run this section for parallel jobs on cloud. Setting cloud environment is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_catalog(\n",
    "    config_json: InputPath(\"json\"),\n",
    "    catalog_csv: OutputPath(str),\n",
    "    picks_csv: OutputPath(str),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from minio import Minio\n",
    "\n",
    "    minioClient = Minio(s3_url, access_key='minio', secret_key='minio123', secure=secure)\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    objects = minioClient.list_objects(bucket_name, prefix=config[\"region\"], recursive=True)\n",
    "\n",
    "    tmp_path = lambda x: os.path.join(\"/tmp/\", x)\n",
    "    for obj in objects:\n",
    "        print(obj._object_name)\n",
    "        minioClient.fget_object(bucket_name, obj._object_name, tmp_path(obj._object_name.split(\"/\")[-1]))\n",
    "\n",
    "    files_catalog = sorted(glob(tmp_path(\"catalog_*.csv\")))\n",
    "    files_picks = sorted(glob(tmp_path(\"picks_*.csv\")))\n",
    "\n",
    "    if len(files_catalog) > 0:\n",
    "        catalog_list = []\n",
    "        for f in files_catalog:\n",
    "            tmp = pd.read_csv(f, sep=\"\\t\", dtype=str)\n",
    "            catalog_list.append(tmp)\n",
    "        merged_catalog = pd.concat(catalog_list).sort_values(by=\"time\")\n",
    "        merged_catalog.to_csv(tmp_path(\"merged_catalog.csv\"), sep=\"\\t\", index=False)\n",
    "        minioClient.fput_object(bucket_name, f\"{config['region']}/merged_catalog.csv\", tmp_path(\"merged_catalog.csv\"))\n",
    "\n",
    "        pick_list = []\n",
    "        for f in files_picks:\n",
    "            tmp = pd.read_csv(f, sep=\"\\t\", dtype=str)\n",
    "            pick_list.append(tmp)\n",
    "        merged_picks = pd.concat(pick_list).sort_values(by=\"timestamp\")\n",
    "        merged_picks.to_csv(tmp_path(\"merged_picks.csv\"), sep=\"\\t\", index=False)\n",
    "        minioClient.fput_object(bucket_name, f\"{config['region']}/merged_picks.csv\", tmp_path(\"merged_picks.csv\"))\n",
    "\n",
    "        with open(catalog_csv, \"w\") as fout:\n",
    "            with open(tmp_path(\"merged_catalog.csv\"), \"r\") as fin:\n",
    "                for line in fin:\n",
    "                    fout.write(line)\n",
    "        with open(picks_csv, \"w\") as fout:\n",
    "            with open(tmp_path(\"merged_picks.csv\"), \"r\") as fin:\n",
    "                for line in fin:\n",
    "                    fout.write(line)\n",
    "    else:\n",
    "        with open(catalog_csv, \"w\") as fout:\n",
    "            pass\n",
    "        print(\"No catalog.csv found!\")\n",
    "        with open(picks_csv, \"w\") as fout:\n",
    "            pass\n",
    "        print(\"No picks.csv found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_local\n",
    "# combine_catalog(root_dir(\"config.json\"), root_dir(\"combined_catalog.csv\"), root_dir(\"combined_picks.csv\"), bucket_name=\"catalogs\", s3_url=\"localhost:9000\", secure=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_op = comp.func_to_container_op(\n",
    "    merge_catalog,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\"pandas\", \"minio\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define QuakeFlow pipeline\n",
    "@dsl.pipeline(name='QuakeFlow', description='')\n",
    "def quakeflow_pipeline(\n",
    "    data_path: str = \"/tmp/\",\n",
    "    num_parallel=0,\n",
    "    bucket_catalog: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = False,\n",
    "):\n",
    "\n",
    "    config = config_op(num_parallel)\n",
    "\n",
    "    events = download_events_op(config.outputs[\"config_json\"])\n",
    "\n",
    "    stations = download_stations_op(config.outputs[\"config_json\"])\n",
    "\n",
    "    with kfp.dsl.ParallelFor(config.outputs[\"output\"]) as i:\n",
    "\n",
    "        vop_ = dsl.VolumeOp(\n",
    "            name=f\"Create volume\", resource_name=f\"data-volume-{str(i)}\", size=\"20Gi\", modes=dsl.VOLUME_MODE_RWO\n",
    "        ).set_retry(3)\n",
    "\n",
    "        download_op_ = (\n",
    "            download_waveform_op(\n",
    "                i,\n",
    "                config.outputs[\"index_json\"],\n",
    "                config.outputs[\"config_json\"],\n",
    "                config.outputs[\"datetime_json\"],\n",
    "                stations.outputs[\"station_pkl\"],\n",
    "                data_path=data_path,\n",
    "                bucket_name=f\"waveforms\",\n",
    "                # s3_url=s3_url,\n",
    "                s3_url=\"quakeflow-minio.default.svc.cluster.local:9000\",\n",
    "                secure=secure,\n",
    "            )\n",
    "            .add_pvolumes({data_path: vop_.volume})\n",
    "            .set_cpu_request(\"860m\") #2CPU per node\n",
    "            .set_retry(3)\n",
    "            .set_display_name('Download Waveforms')\n",
    "        )\n",
    "        download_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "        phasenet2gamma_op_ = (\n",
    "            phasenet2gamma_op(\n",
    "                i,\n",
    "                config.outputs[\"index_json\"],\n",
    "                config.outputs[\"config_json\"],\n",
    "                station_csv=stations.outputs[\"station_csv\"],\n",
    "                data_path=download_op_.outputs[\"Output\"],\n",
    "                data_list=download_op_.outputs[\"fname_csv\"],\n",
    "                bucket_name=f\"catalogs\",\n",
    "                s3_url=s3_url,\n",
    "                secure=secure,\n",
    "            ).add_pvolumes({data_path: download_op_.pvolume})\n",
    "            .set_cpu_request(\"860m\")\n",
    "            # .set_memory_request(\"2500M\") #6GB per node\n",
    "            .set_display_name('PhaseNet + GaMMA')\n",
    "        )\n",
    "        phasenet2gamma_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "    merge_op_ = merge_op(config.outputs[\"config_json\"], bucket_name=f\"catalogs\", s3_url=s3_url, secure=secure).after(\n",
    "        phasenet2gamma_op_\n",
    "    )\n",
    "    merge_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    vop_.delete().after(merge_op_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "helm install quakeflow-minio --set accessKey.password=minio --set secretKey.password=minio123 --set persistence.size=1T  bitnami/minio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to get GCP access token: File /Users/weiqiang/.dotbot/cloud/quakeflow_zhuwq.json was not found.\n"
     ]
    },
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPSConnectionPool(host='accounts.google.com', port=443): Max retries exceeded with url: https://accounts.google.com/InteractiveLogin?continue=https://us-west1.notebooks.cloud.google.com/_signin?continue%3Dhttps%253A%252F%252F66bece58bfa6ae5b-dot-us-west1.pipelines.googleusercontent.com%252Fapis%252Fv1beta1%252Fhealthz%26endpoint%3D66bece58bfa6ae5b&ifkv=ASSHykqaLcG5-hRdvW4jr1MawpLoWrtuKO68ICv0lj-ULtFBDMWYvC3uMhA-ChYQGK2JhK4Uxlmbxg (Caused by ResponseError('too many redirects'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m      8\u001b[0m arguments \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_parallel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecure\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_local:\n\u001b[0;32m---> 18\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mkfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m66bece58bfa6ae5b-dot-us-west1.pipelines.googleusercontent.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mCompiler()\u001b[38;5;241m.\u001b[39mcompile(pipeline_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(experiment_name))\n\u001b[1;32m     20\u001b[0m     results \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_run_from_pipeline_func(\n\u001b[1;32m     21\u001b[0m         pipeline_func, experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name, run_name\u001b[38;5;241m=\u001b[39mrun_name, arguments\u001b[38;5;241m=\u001b[39marguments\n\u001b[1;32m     22\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp/_client.py:196\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, host, client_id, namespace, other_client_id, other_client_secret, existing_token, cookies, proxy, ssl_ca_cert, kube_context, credentials, ui_host)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_api \u001b[38;5;241m=\u001b[39m kfp_server_api\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mPipelineUploadServiceApi(\n\u001b[1;32m    193\u001b[0m     api_client)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_healthz_api \u001b[38;5;241m=\u001b[39m kfp_server_api\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mhealthz_service_api\u001b[38;5;241m.\u001b[39mHealthzServiceApi(\n\u001b[1;32m    195\u001b[0m     api_client)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_setting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_kfp_healthz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmulti_user \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Client\u001b[38;5;241m.\u001b[39mNAMESPACE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp/_client.py:410\u001b[0m, in \u001b[0;36mClient.get_kfp_healthz\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed getting healthz endpoint after \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m attempts.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    408\u001b[0m             max_attempts))\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_healthz_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_healthz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# ApiException, including network errors, is the only type that may\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# recover after retry.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/api/healthz_service_api.py:63\u001b[0m, in \u001b[0;36mHealthzServiceApi.get_healthz\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get healthz data.  # noqa: E501\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m:rtype: ApiGetHealthzResponse\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_healthz_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/api/healthz_service_api.py:134\u001b[0m, in \u001b[0;36mHealthzServiceApi.get_healthz_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Authentication setting\u001b[39;00m\n\u001b[1;32m    132\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/apis/v1beta1/healthz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApiGetHealthzResponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/api_client.py:364\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    372\u001b[0m                                                method, path_params,\n\u001b[1;32m    373\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m                                                _request_timeout,\n\u001b[1;32m    382\u001b[0m                                                _host))\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/api_client.py:181\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/api_client.py:389\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mHEAD(url,\n\u001b[1;32m    396\u001b[0m                                  query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    397\u001b[0m                                  _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    398\u001b[0m                                  _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    399\u001b[0m                                  headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/rest.py:230\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[0;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGET\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m         _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/kfp_server_api/rest.py:208\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ApiException(status\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reason\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# For `GET`, `HEAD`\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib3\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    214\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/request.py:77\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_url\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     82\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     83\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/request.py:99\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[1;32m     97\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/poolmanager.py:421\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    418\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedirecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, redirect_location)\n\u001b[1;32m    420\u001b[0m response\u001b[38;5;241m.\u001b[39mdrain_conn()\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredirect_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/poolmanager.py:421\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    418\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedirecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, redirect_location)\n\u001b[1;32m    420\u001b[0m response\u001b[38;5;241m.\u001b[39mdrain_conn()\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredirect_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/poolmanager.py:421\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    418\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedirecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, redirect_location)\n\u001b[1;32m    420\u001b[0m response\u001b[38;5;241m.\u001b[39mdrain_conn()\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredirect_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/poolmanager.py:408\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    405\u001b[0m             kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpop(header, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries\u001b[38;5;241m.\u001b[39mraise_on_redirect:\n",
      "File \u001b[0;32m~/miniconda3/envs/quakeflow/lib/python3.8/site-packages/urllib3/util/retry.py:594\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    583\u001b[0m new_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew(\n\u001b[1;32m    584\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal,\n\u001b[1;32m    585\u001b[0m     connect\u001b[38;5;241m=\u001b[39mconnect,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    596\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='accounts.google.com', port=443): Max retries exceeded with url: https://accounts.google.com/InteractiveLogin?continue=https://us-west1.notebooks.cloud.google.com/_signin?continue%3Dhttps%253A%252F%252F66bece58bfa6ae5b-dot-us-west1.pipelines.googleusercontent.com%252Fapis%252Fv1beta1%252Fhealthz%26endpoint%3D66bece58bfa6ae5b&ifkv=ASSHykqaLcG5-hRdvW4jr1MawpLoWrtuKO68ICv0lj-ULtFBDMWYvC3uMhA-ChYQGK2JhK4Uxlmbxg (Caused by ResponseError('too many redirects'))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow_zhuwq.json\"\n",
    "experiment_name = 'QuakeFlow'\n",
    "pipeline_func = quakeflow_pipeline\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {\n",
    "    \"data_path\": \"/tmp\",\n",
    "    \"num_parallel\": 0,\n",
    "    \"bucket_catalog\": \"catalogs\",\n",
    "    \"s3_url\": \"minio-service:9000\",\n",
    "    #  \"s3_url\": \"quakeflow-minio.default.svc.cluster.local:9000\",\n",
    "    \"secure\": False,\n",
    "}\n",
    "\n",
    "if not run_local:\n",
    "    client = kfp.Client(host=\"66bece58bfa6ae5b-dot-us-west1.pipelines.googleusercontent.com\")\n",
    "    kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "    results = client.create_run_from_pipeline_func(\n",
    "        pipeline_func, experiment_name=experiment_name, run_name=run_name, arguments=arguments\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
