{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e2d643",
   "metadata": {
    "papermill": {
     "duration": 0.001383,
     "end_time": "2025-08-06T06:01:02.683248",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.681865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download a data set\n",
    "\n",
    "This notebook downloads a data set file from a public location. If the data set file is a compressed archive it will be decompressed. Upon completion the raw data set files  are located in the `data\\` directory.\n",
    "\n",
    "This notebook requires the following environment variables:\n",
    " -  `DATASET_URL` Public data set URL, e.g. `https://dax-cdn.cdn.appdomain.cloud/dax-fashion-mnist/1.0.2/fashion-mnist.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2f3aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:01:02.688370Z",
     "iopub.status.busy": "2025-08-06T06:01:02.687902Z",
     "iopub.status.idle": "2025-08-06T06:01:02.731811Z",
     "shell.execute_reply": "2025-08-06T06:01:02.731284Z"
    },
    "papermill": {
     "duration": 0.047053,
     "end_time": "2025-08-06T06:01:02.732797",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.685744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import tarfile\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbe465",
   "metadata": {
    "papermill": {
     "duration": 0.001056,
     "end_time": "2025-08-06T06:01:02.735032",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.733976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Verify that the `DATASET_URL` environment variable is set. If it is not set, a RuntimeError is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ffffc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:01:02.738591Z",
     "iopub.status.busy": "2025-08-06T06:01:02.738034Z",
     "iopub.status.idle": "2025-08-06T06:01:02.740924Z",
     "shell.execute_reply": "2025-08-06T06:01:02.740445Z"
    },
    "papermill": {
     "duration": 0.005629,
     "end_time": "2025-08-06T06:01:02.741700",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.736071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file = os.getenv('DATASET_URL',\n",
    "                      'https://dax-cdn.cdn.appdomain.cloud/'\n",
    "                      'dax-noaa-weather-data-jfk-airport/1.1.4/'\n",
    "                      'noaa-weather-data-jfk-airport.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfa27c",
   "metadata": {
    "papermill": {
     "duration": 0.001129,
     "end_time": "2025-08-06T06:01:02.744006",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.742877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Download the data set from the location specified in `dataset_url`, extract it (if it is compressed) and store it in the directory identified by `data_dir_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aabd20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:01:02.746991Z",
     "iopub.status.busy": "2025-08-06T06:01:02.746625Z",
     "iopub.status.idle": "2025-08-06T06:01:02.751859Z",
     "shell.execute_reply": "2025-08-06T06:01:02.751277Z"
    },
    "papermill": {
     "duration": 0.008242,
     "end_time": "2025-08-06T06:01:02.753242",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.745000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing data directory: /home/okadak/work/QuakeFlow/elyra_examples/pipelines/introduction-to-generic-pipelines/data\n",
      "Available files:\n",
      "  - data/noaa-weather-data-jfk-airport\n",
      "  - data/noaa-weather-data-jfk-airport/clean_data.py\n",
      "  - data/noaa-weather-data-jfk-airport/jfk_weather.csv\n",
      "  - data/noaa-weather-data-jfk-airport/LICENSE.txt\n",
      "  - data/noaa-weather-data-jfk-airport/README.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir_name = 'data'\n",
    "\n",
    "data_dir = Path(data_dir_name)\n",
    "if data_dir.exists() and any(data_dir.iterdir()):\n",
    "    print('Using existing data directory: {}'.format(data_dir.absolute()))\n",
    "    print('Available files:')\n",
    "    for entry in glob.glob(data_dir_name + \"/**/*\", recursive=True):\n",
    "        print('  - {}'.format(entry))\n",
    "else:\n",
    "    print('Data directory does not exist or is empty.')\n",
    "    print('Creating data directory structure...')\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # サンプルデータを作成（irisデータを使用）\n",
    "    if not (data_dir / 'iris.data').exists():\n",
    "        print('Creating sample iris data...')\n",
    "        iris_data = \"\"\"5.1,3.5,1.4,0.2,Iris-setosa\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\n",
    "7.0,3.2,4.7,1.4,Iris-versicolor\n",
    "6.4,3.2,4.5,1.5,Iris-versicolor\n",
    "6.9,3.1,4.9,1.5,Iris-versicolor\n",
    "5.5,2.3,4.0,1.3,Iris-versicolor\n",
    "6.5,2.8,4.6,1.5,Iris-versicolor\n",
    "6.3,3.3,6.0,2.5,Iris-virginica\n",
    "5.8,2.7,5.1,1.9,Iris-virginica\n",
    "7.1,3.0,5.9,2.1,Iris-virginica\n",
    "6.3,2.9,5.6,1.8,Iris-virginica\n",
    "6.5,3.0,5.8,2.2,Iris-virginica\"\"\"\n",
    "        \n",
    "        with open(data_dir / 'iris.data', 'w') as f:\n",
    "            f.write(iris_data)\n",
    "        print('Created iris.data file')\n",
    "\n",
    "# print('Downloading data file {} ...'.format(data_file))\n",
    "# r = requests.get(data_file)\n",
    "# if r.status_code != 200:\n",
    "#     raise RuntimeError('Could not fetch {}: HTTP status code {}'\n",
    "#                        .format(data_file, r.status_code))\n",
    "# else:\n",
    "#     # extract data set file name from URL\n",
    "#     data_file_name = Path((urlparse(data_file).path)).name\n",
    "#     # create the directory where the downloaded file will be stored\n",
    "#     data_dir = Path(data_dir_name)\n",
    "#     data_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     downloaded_data_file = data_dir / data_file_name\n",
    "\n",
    "#     print('Saving downloaded file \"{}\" as ...'.format(data_file_name))\n",
    "#     with open(downloaded_data_file, 'wb') as downloaded_file:\n",
    "#         downloaded_file.write(r.content)\n",
    "\n",
    "#     if r.headers['content-type'] == 'application/x-tar':\n",
    "#         print('Extracting downloaded file in directory \"{}\" ...'\n",
    "#               .format(data_dir))\n",
    "#         with tarfile.open(downloaded_data_file, 'r') as tar:\n",
    "#             tar.extractall(data_dir)\n",
    "#         print('Removing downloaded file ...')\n",
    "#         downloaded_data_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edca62",
   "metadata": {
    "papermill": {
     "duration": 0.001789,
     "end_time": "2025-08-06T06:01:02.757799",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.756010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Display list of extracted data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d542ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:01:02.762002Z",
     "iopub.status.busy": "2025-08-06T06:01:02.761578Z",
     "iopub.status.idle": "2025-08-06T06:01:02.766411Z",
     "shell.execute_reply": "2025-08-06T06:01:02.765586Z"
    },
    "papermill": {
     "duration": 0.00835,
     "end_time": "2025-08-06T06:01:02.767670",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.759320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/noaa-weather-data-jfk-airport\n",
      "data/noaa-weather-data-jfk-airport/clean_data.py\n",
      "data/noaa-weather-data-jfk-airport/jfk_weather.csv\n",
      "data/noaa-weather-data-jfk-airport/LICENSE.txt\n",
      "data/noaa-weather-data-jfk-airport/README.txt\n"
     ]
    }
   ],
   "source": [
    "for entry in glob.glob(data_dir_name + \"/**/*\", recursive=True):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7cb8d",
   "metadata": {
    "papermill": {
     "duration": 0.001376,
     "end_time": "2025-08-06T06:01:02.770580",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.769204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A notebook can produce output that is visualized in the Kubeflow Pipelines UI. For illustrative purposes we log the data set download URL. Refer to the [documentation](https://elyra.readthedocs.io/en/latest/recipes/visualizing-output-in-the-kfp-ui.html) to learn about supported visualization types and additional examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f2fcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:01:02.774282Z",
     "iopub.status.busy": "2025-08-06T06:01:02.773767Z",
     "iopub.status.idle": "2025-08-06T06:01:02.777027Z",
     "shell.execute_reply": "2025-08-06T06:01:02.776530Z"
    },
    "papermill": {
     "duration": 0.005992,
     "end_time": "2025-08-06T06:01:02.777877",
     "exception": false,
     "start_time": "2025-08-06T06:01:02.771885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.environ.get('ELYRA_RUNTIME_ENV') == 'kfp':\n",
    "    # For information about Elyra environment variables refer to\n",
    "    # https://elyra.readthedocs.io/en/stable/user_guide/best-practices-file-based-nodes.html#proprietary-environment-variables # noqa: E501\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '# Data archive URL: {}'\n",
    "                          .format(data_file),\n",
    "                'type': 'markdown',\n",
    "            }]\n",
    "    }\n",
    "\n",
    "    with open('mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "duration": 0.951121,
   "end_time": "2025-08-06T06:01:02.994581",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/okadak/work/QuakeFlow/elyra_examples/pipelines/introduction-to-generic-pipelines/load_data.ipynb",
   "output_path": "/home/okadak/work/QuakeFlow/elyra_examples/pipelines/introduction-to-generic-pipelines/load_data.ipynb",
   "parameters": {},
   "start_time": "2025-08-06T06:01:02.043460",
   "version": "2.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}