{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f9fe4e",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815a1d9",
   "metadata": {
    "papermill": {
     "duration": 0.004738,
     "end_time": "2025-08-06T05:29:36.725252",
     "exception": false,
     "start_time": "2025-08-06T05:29:36.720514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cleaning NOAA Weather Data of JFK Airport (New York)\n",
    "\n",
    "This notebook relates to the NOAA Weather Dataset - JFK Airport (New York). The dataset contains 114,546 hourly observations of 12 local climatological variables (such as temperature and wind speed) collected at JFK airport. This dataset can be obtained for free from the IBM Developer [Data Asset Exchange](https://developer.ibm.com/exchanges/data/all/jfk-weather-data/).\n",
    "\n",
    "In this notebook, we clean the raw dataset by:\n",
    "* removing redundant columns and preserving only key numeric columns\n",
    "* converting and cleaning data where required\n",
    "* creating a fixed time interval between observations (this aids with later time-series analysis)\n",
    "* filling missing values\n",
    "* encoding certain weather features\n",
    "\n",
    "### Table of Contents:\n",
    "* [1. Read the Raw Data](#cell1)\n",
    "* [2. Clean the Data](#cell2)\n",
    "  * [2.1 Select data columns](#cell3)\n",
    "  * [2.2 Clean up precipitation column](#cell4)\n",
    "  * [2.3 Convert columns to numerical types](#cell5)\n",
    "  * [2.4 Reformat and process data](#cell6)\n",
    "  * [2.5 Create a fixed interval dataset](#cell7)\n",
    "  * [2.6 Feature encoding](#cell8)\n",
    "  * [2.7 Rename columns](#cell9)\n",
    "* [3. Save the Cleaned Data](#cell10)\n",
    "* [Authors](#authors)\n",
    "\n",
    "#### Import required modules\n",
    "\n",
    "Import and configure the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822c6784",
   "metadata": {
    "papermill": {
     "duration": 0.970143,
     "end_time": "2025-08-06T05:29:37.698768",
     "exception": false,
     "start_time": "2025-08-06T05:29:36.728625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install PyGithub pandas > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313c403a",
   "metadata": {
    "papermill": {
     "duration": 0.209332,
     "end_time": "2025-08-06T05:29:37.912075",
     "exception": false,
     "start_time": "2025-08-06T05:29:37.702743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define required imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# These set pandas max column and row display in the notebook\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93fab8",
   "metadata": {
    "papermill": {
     "duration": 0.003039,
     "end_time": "2025-08-06T05:29:37.918543",
     "exception": false,
     "start_time": "2025-08-06T05:29:37.915504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell1\"></a>\n",
    "\n",
    "### 1. Read the Raw Data\n",
    "\n",
    "We start by reading in the raw dataset, displaying the first few rows of the dataframe, and taking a look at the columns and column types present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd94807",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3127286",
   "metadata": {
    "papermill": {
     "duration": 0.295562,
     "end_time": "2025-08-06T05:29:38.216982",
     "exception": true,
     "start_time": "2025-08-06T05:29:37.921420",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>AWND</th>\n",
       "      <th>PGTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>35</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>38</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  PRCP  SNOW  TMAX  TMIN  AWND  PGTM\n",
       "0 2010-01-01   0.0   0.0    45    32   8.5  1200\n",
       "1 2010-01-02   0.1   0.0    42    28  12.3  1400\n",
       "2 2010-01-03   0.0   0.0    48    35   6.8  1100\n",
       "3 2010-01-04   0.2   0.0    44    30   9.2  1300\n",
       "4 2010-01-05   0.0   0.0    50    38   7.1  1200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('data/noaa-weather-data-jfk-airport/jfk_weather.csv',\n",
    "                       parse_dates=['DATE'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79979e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE    datetime64[ns]\n",
       "PRCP           float64\n",
       "SNOW           float64\n",
       "TMAX             int64\n",
       "TMIN             int64\n",
       "AWND           float64\n",
       "PGTM             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c006c31",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell2\"></a>\n",
    "\n",
    "### 2. Clean the Data\n",
    "\n",
    "As you can see above, there are a lot of fields which are non-numerical - usually these will be fields that contain text or categorical data, e.g. `HOURLYSKYCONDITIONS`.\n",
    "\n",
    "There are also fields - such as the main temperature field of interest `HOURLYDRYBULBTEMPF` - that we expect to be numerical, but are instead `object` type. This often indicates that there may be missing (or `null`) values, or some other unusual readings that we may have to deal with (since otherwise the field would have been fully parsed as a numerical data type).\n",
    "\n",
    "In addition, some fields relate to hourly observations, while others relate to daily or monthly intervals. For purposes of later exploratory data analysis, we will restrict the dataset to a certain subset  of numerical fields that relate to hourly observations.\n",
    "\n",
    "In this section, we refer to the [NOAA Local Climatological Data Documentation](https://data.noaa.gov/dataset/dataset/u-s-local-climatological-data-lcd/resource/ee7381ea-647a-434f-8cfa-81202b9b4c05) to describe the fields and meaning of various values.\n",
    "\n",
    "<a id=\"cell3\"></a>\n",
    "#### 2.1 Select data columns\n",
    "\n",
    "First, we select only the subset of data columns of interest and inspect the column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "191f1583",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['HOURLYVISIBILITY', 'HOURLYDRYBULBTEMPF', 'HOURLYWETBULBTEMPF', 'HOURLYDewPointTempF', 'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYStationPressure', 'HOURLYPressureTendency', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m      2\u001b[0m column_subset \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOURLYVISIBILITY\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOURLYAltimeterSetting\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Filter dataset to relevant columns\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m hourly_data \u001b[38;5;241m=\u001b[39m \u001b[43mraw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_subset\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Set date index\u001b[39;00m\n\u001b[1;32m     21\u001b[0m hourly_data \u001b[38;5;241m=\u001b[39m hourly_data\u001b[38;5;241m.\u001b[39mset_index(pd\u001b[38;5;241m.\u001b[39mDatetimeIndex(hourly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/elyra-env/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/elyra-env/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/elyra-env/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['HOURLYVISIBILITY', 'HOURLYDRYBULBTEMPF', 'HOURLYWETBULBTEMPF', 'HOURLYDewPointTempF', 'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYStationPressure', 'HOURLYPressureTendency', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting'] not in index\""
     ]
    }
   ],
   "source": [
    "# Choose what columns to import from raw data\n",
    "column_subset = [\n",
    "    'DATE',\n",
    "    'HOURLYVISIBILITY',\n",
    "    'HOURLYDRYBULBTEMPF',\n",
    "    'HOURLYWETBULBTEMPF',\n",
    "    'HOURLYDewPointTempF',\n",
    "    'HOURLYRelativeHumidity',\n",
    "    'HOURLYWindSpeed',\n",
    "    'HOURLYWindDirection',\n",
    "    'HOURLYStationPressure',\n",
    "    'HOURLYPressureTendency',\n",
    "    'HOURLYSeaLevelPressure',\n",
    "    'HOURLYPrecip',\n",
    "    'HOURLYAltimeterSetting'\n",
    "]\n",
    "\n",
    "# Filter dataset to relevant columns\n",
    "hourly_data = raw_data[column_subset]\n",
    "# Set date index\n",
    "hourly_data = hourly_data.set_index(pd.DatetimeIndex(hourly_data['DATE']))\n",
    "hourly_data.drop(['DATE'], axis=1, inplace=True)\n",
    "hourly_data.replace(to_replace='*', value=np.nan, inplace=True)\n",
    "hourly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac52660",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hourly_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhourly_data\u001b[49m\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hourly_data' is not defined"
     ]
    }
   ],
   "source": [
    "hourly_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f32883",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell4\"></a>\n",
    "#### 2.2 Clean up precipitation column\n",
    "\n",
    "From the dataframe preview above, we can see that the column `HOURLYPrecip` - which is the hourly measure of precipitation levels - contains both `NaN` and `T` values. `T` specifies *trace amounts of precipitation*, while `NaN` means *not a number*, and is used to denote missing values.\n",
    "\n",
    "We can also inspect the unique values present for the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ef9227",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hourly_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhourly_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOURLYPrecip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hourly_data' is not defined"
     ]
    }
   ],
   "source": [
    "hourly_data['HOURLYPrecip'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae7354",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can see that some values end with an `s` (indicating snow), while there is a strange value `0.020.01s` which appears to be an error of some sort. To deal with `T` values, we will set the observation to be `0`. We will also replace the erroneous value `0.020.01s` with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad111898",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hourly_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fix imported data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhourly_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOURLYPrecip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(to_replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.00\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m hourly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOURLYPrecip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.020.01s\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hourly_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Fix imported data\n",
    "hourly_data['HOURLYPrecip'].replace(to_replace='T', value='0.00', inplace=True)\n",
    "hourly_data['HOURLYPrecip'].replace('0.020.01s', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38b44a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell5\"></a>\n",
    "#### 2.3 Convert columns to numerical types\n",
    "\n",
    "Next, we will convert string columns that refer to numerical values to numerical types. For columns such as `HOURLYPrecip`, we first also drop the non-numerical parts of the value (the `s` character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f692ac9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hourly_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert columns to float32 datatype\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m messy_columns:\n\u001b[0;32m----> 6\u001b[0m     hourly_data[i] \u001b[38;5;241m=\u001b[39m \u001b[43mhourly_data\u001b[49m[i]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^0-9,.-]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, x)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)\u001b[38;5;241m.\u001b[39mastype((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hourly_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Set of columns to convert\n",
    "messy_columns = column_subset[1:]\n",
    "\n",
    "# Convert columns to float32 datatype\n",
    "for i in messy_columns:\n",
    "    hourly_data[i] = hourly_data[i].apply(\n",
    "        lambda x: re.sub('[^0-9,.-]', '', x)\n",
    "        if type(x) == str else x).replace('', np.nan).astype(('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4d0cb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can now see that all fields have numerical data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e38fa8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hourly_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhourly_data\u001b[49m\u001b[38;5;241m.\u001b[39minfo())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      3\u001b[0m hourly_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hourly_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(hourly_data.info())\n",
    "print()\n",
    "hourly_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a8436",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell6\"></a>\n",
    "#### 2.4 Reformat and process data\n",
    "\n",
    "Next, we will clean up some of the data columns to ensure their values fall within the parameters defined by the NOAA documentation (referred to above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57803440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.483326Z",
     "iopub.status.busy": "2025-05-02T05:11:40.482702Z",
     "iopub.status.idle": "2025-05-02T05:11:40.535034Z",
     "shell.execute_reply": "2025-05-02T05:11:40.534381Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the summary statistics for each column\n",
    "hourly_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5e74d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "According to the documentation, the `HOURLYPressureTendency` field should be an integer value in the range `[0, 8]`. Let's check if this condition holds for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87dd506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.553440Z",
     "iopub.status.busy": "2025-05-02T05:11:40.553206Z",
     "iopub.status.idle": "2025-05-02T05:11:40.563269Z",
     "shell.execute_reply": "2025-05-02T05:11:40.562658Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if categorical variable HOURLYPressureTendency ever has a non-integer\n",
    "# entry outside the bounds of 0-8\n",
    "cond =\\\n",
    "    len(hourly_data[~hourly_data['HOURLYPressureTendency'].isin(\n",
    "        list(range(0, 9)) + [np.nan])])\n",
    "\n",
    "print('Hourly Pressure Tendency should be between 0 and 8: {}'\n",
    "      .format(cond == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9814bf2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The `HOURLYVISIBILITY` should be an integer in the range `[0, 10]`. Let's check this condition too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a6d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.582287Z",
     "iopub.status.busy": "2025-05-02T05:11:40.581559Z",
     "iopub.status.idle": "2025-05-02T05:11:40.591610Z",
     "shell.execute_reply": "2025-05-02T05:11:40.590935Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hourly Visibility should be between 0 and 10\n",
    "hourly_data[(hourly_data['HOURLYVISIBILITY'] < 0) | (hourly_data['HOURLYVISIBILITY'] > 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda2d9d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We find that a couple of observations fall outside the range. These must be spurious data observations and we handle them by replacing them with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8506e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.611742Z",
     "iopub.status.busy": "2025-05-02T05:11:40.611121Z",
     "iopub.status.idle": "2025-05-02T05:11:40.616925Z",
     "shell.execute_reply": "2025-05-02T05:11:40.616355Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace any hourly visibility figure outside these bounds with nan\n",
    "hourly_data.loc[hourly_data['HOURLYVISIBILITY'] > 10, 'HOURLYVISIBILITY'] = np.nan\n",
    "\n",
    "# Hourly Visibility should be between 0 and 10\n",
    "cond = len(hourly_data[(hourly_data['HOURLYVISIBILITY'] < 0) | (hourly_data['HOURLYVISIBILITY'] > 10)])\n",
    "\n",
    "print('Hourly Visibility should be between 0 and 10: {}'.format(cond == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6769c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Finally, we check if there are any duplicates with respect to our `DATE` index and check furthermore that our dates are in the correct order (that is, strictly increasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c97de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.637045Z",
     "iopub.status.busy": "2025-05-02T05:11:40.636609Z",
     "iopub.status.idle": "2025-05-02T05:11:40.643046Z",
     "shell.execute_reply": "2025-05-02T05:11:40.642412Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cond = len(hourly_data[hourly_data.index.duplicated()].sort_index())\n",
    "print('Date index contains no duplicate entries: {}'.format(cond == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbd244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.654764Z",
     "iopub.status.busy": "2025-05-02T05:11:40.654233Z",
     "iopub.status.idle": "2025-05-02T05:11:40.658377Z",
     "shell.execute_reply": "2025-05-02T05:11:40.657771Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure time index is sorted and increasing\n",
    "print('Date index is strictly increasing: {}'\n",
    "      .format(hourly_data.index.is_monotonic_increasing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072b427",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell7\"></a>\n",
    "#### 2.5 Create a fixed interval dataset\n",
    "\n",
    "Most time-series analysis requires (or certainly works much better with) data that has fixed measurement intervals. As you may have noticed from the various data samples above, the measurement intervals for this dataset are not exactly hourly. So, we will use `Pandas`' resampling functionality to create a dataset that has exact hourly measurement intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eef6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.679972Z",
     "iopub.status.busy": "2025-05-02T05:11:40.679234Z",
     "iopub.status.idle": "2025-05-02T05:11:40.691477Z",
     "shell.execute_reply": "2025-05-02T05:11:40.690634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resample (downsample) to hourly rows (we're shifting everything up by 9 minutes!)\n",
    "hourly_data = hourly_data.resample('60min').last().shift(periods=1)  # noqa Note: use resample('60min', base=51) to resample on the 51st of every hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76e9bf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We will now also replace missing values. For numerical values, we will linearly interpolate between the previous and next valid obvservations. For the categorical `HOURLYPressureTendency` field, we will replace missing values with the last valid observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289cde7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.712605Z",
     "iopub.status.busy": "2025-05-02T05:11:40.712091Z",
     "iopub.status.idle": "2025-05-02T05:11:40.747554Z",
     "shell.execute_reply": "2025-05-02T05:11:40.746813Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_data['HOURLYPressureTendency'] =\\\n",
    "    hourly_data['HOURLYPressureTendency'].fillna(method='ffill')  # fill with last valid observation\n",
    "hourly_data = hourly_data.interpolate(method='linear')  # interpolate missing values\n",
    "hourly_data.drop(hourly_data.index[0], inplace=True)  # drop first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99295ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.759328Z",
     "iopub.status.busy": "2025-05-02T05:11:40.758648Z",
     "iopub.status.idle": "2025-05-02T05:11:40.773611Z",
     "shell.execute_reply": "2025-05-02T05:11:40.772985Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hourly_data.info())\n",
    "print()\n",
    "hourly_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf6f8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell8\"></a>\n",
    "#### 2.6 Feature encoding\n",
    "\n",
    "The final pre-processing step we will perform will be to handle two of our columns in a special way in order to correctly encode these features. They are:\n",
    "\n",
    "1. `HOURLYWindDirection` - wind direction\n",
    "2. `HOURLYPressureTendency` - an indicator of pressure changes\n",
    "\n",
    "For `HOURLYWindDirection`, we encode the raw feature value as two new values, which measure the cyclical nature of wind direction - that is, we are encoding the compass-point nature of wind direction measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1fb39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.794982Z",
     "iopub.status.busy": "2025-05-02T05:11:40.794251Z",
     "iopub.status.idle": "2025-05-02T05:11:40.800909Z",
     "shell.execute_reply": "2025-05-02T05:11:40.800203Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform HOURLYWindDirection into a cyclical variable using sin and cos transforms\n",
    "hourly_data['HOURLYWindDirectionSin'] = np.sin(hourly_data['HOURLYWindDirection'] * (2. * np.pi / 360))\n",
    "hourly_data['HOURLYWindDirectionCos'] = np.cos(hourly_data['HOURLYWindDirection'] * (2. * np.pi / 360))\n",
    "hourly_data.drop(['HOURLYWindDirection'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e24f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "For `HOURLYPressureTendency`, the feature value is in fact a `categorical` feature with three levels:\n",
    "* `0-3` indicates an increase in pressure over the previous 3 hours\n",
    "* `4` indicates no change during the previous 3 hours\n",
    "* `5-8` indicates a decrease over the previous 3 hours\n",
    "\n",
    "Hence, we encode this feature into 3 dummy values representing these 3 potential states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3cafa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.821461Z",
     "iopub.status.busy": "2025-05-02T05:11:40.820709Z",
     "iopub.status.idle": "2025-05-02T05:11:40.852770Z",
     "shell.execute_reply": "2025-05-02T05:11:40.852079Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform HOURLYPressureTendency into 3 dummy variables based on NOAA documentation\n",
    "hourly_data['HOURLYPressureTendencyIncr'] =\\\n",
    "    [1.0 if x in [0, 1, 2, 3]\n",
    "        else 0.0 for x in hourly_data['HOURLYPressureTendency']]  # noqa 0 through 3 indicates an increase in pressure over previous 3 hours\n",
    "hourly_data['HOURLYPressureTendencyDecr'] =\\\n",
    "    [1.0 if x in [5, 6, 7, 8]\n",
    "     else 0.0 for x in hourly_data['HOURLYPressureTendency']]  # noqa 5 through 8 indicates a decrease over previous 3 hours\n",
    "hourly_data['HOURLYPressureTendencyConst'] =\\\n",
    "    [1.0 if x == 4\n",
    "     else 0.0 for x in hourly_data['HOURLYPressureTendency']]  # noqa 4 indicates no change during previous 3 hours\n",
    "hourly_data.drop(['HOURLYPressureTendency'], axis=1, inplace=True)\n",
    "hourly_data['HOURLYPressureTendencyIncr'] =\\\n",
    "    hourly_data['HOURLYPressureTendencyIncr'].astype(('float32'))\n",
    "hourly_data['HOURLYPressureTendencyDecr'] =\\\n",
    "    hourly_data['HOURLYPressureTendencyDecr'].astype(('float32'))\n",
    "hourly_data['HOURLYPressureTendencyConst'] =\\\n",
    "    hourly_data['HOURLYPressureTendencyConst'].astype(('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec2ba3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell9\"></a>\n",
    "#### 2.7 Rename columns\n",
    "\n",
    "Before saving the dataset, we will rename the columns for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc46a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.873635Z",
     "iopub.status.busy": "2025-05-02T05:11:40.872940Z",
     "iopub.status.idle": "2025-05-02T05:11:40.877202Z",
     "shell.execute_reply": "2025-05-02T05:11:40.876651Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d050df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.888884Z",
     "iopub.status.busy": "2025-05-02T05:11:40.888079Z",
     "iopub.status.idle": "2025-05-02T05:11:40.894308Z",
     "shell.execute_reply": "2025-05-02T05:11:40.893586Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the new column names\n",
    "columns_new_name = [\n",
    "    'visibility',\n",
    "    'dry_bulb_temp_f',\n",
    "    'wet_bulb_temp_f',\n",
    "    'dew_point_temp_f',\n",
    "    'relative_humidity',\n",
    "    'wind_speed',\n",
    "    'station_pressure',\n",
    "    'sea_level_pressure',\n",
    "    'precip',\n",
    "    'altimeter_setting',\n",
    "    'wind_direction_sin',\n",
    "    'wind_direction_cos',\n",
    "    'pressure_tendency_incr',\n",
    "    'pressure_tendency_decr',\n",
    "    'pressure_tendency_const'\n",
    "]\n",
    "\n",
    "columns_name_map =\\\n",
    "    {c: columns_new_name[i] for i, c in enumerate(hourly_data.columns)}\n",
    "\n",
    "hourly_data_renamed = hourly_data.rename(columns=columns_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5c2f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.906313Z",
     "iopub.status.busy": "2025-05-02T05:11:40.905385Z",
     "iopub.status.idle": "2025-05-02T05:11:40.919847Z",
     "shell.execute_reply": "2025-05-02T05:11:40.919260Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hourly_data_renamed.info())\n",
    "print()\n",
    "hourly_data_renamed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87058d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.931756Z",
     "iopub.status.busy": "2025-05-02T05:11:40.931525Z",
     "iopub.status.idle": "2025-05-02T05:11:40.938115Z",
     "shell.execute_reply": "2025-05-02T05:11:40.937343Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explore some general information about the dataset\n",
    "print('# of megabytes held by dataframe: {}'.format(\n",
    "      str(round(sys.getsizeof(hourly_data_renamed) / 1000000, 2))))\n",
    "print('# of features: {}'.format(str(hourly_data_renamed.shape[1])))\n",
    "print('# of observations: {}'.format(str(hourly_data_renamed.shape[0])))\n",
    "print('Start date: {}'.format(str(hourly_data_renamed.index[0])))\n",
    "print('End date: {}'.format(str(hourly_data_renamed.index[-1])))\n",
    "print('# of days: {}'.format(\n",
    "      str((hourly_data_renamed.index[-1] - hourly_data_renamed.index[0]).days)))\n",
    "print('# of months: {}'.format(\n",
    "      str(round((hourly_data_renamed.index[-1] - hourly_data_renamed.index[0]).days / 30, 2))))\n",
    "print('# of years: {}'.format(\n",
    "      str(round((hourly_data_renamed.index[-1] - hourly_data_renamed.index[0]).days / 365, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac46ac6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cell10\"></a>\n",
    "\n",
    "### 3. Save the Cleaned Data\n",
    "\n",
    "Finally, we save the cleaned dataset as a Project asset for later re-use. You should see an output like the one below if successful:\n",
    "\n",
    "```\n",
    "{'file_name': 'jfk_weather_cleaned.csv',\n",
    " 'message': 'File saved to project storage.',\n",
    " 'bucket_name': 'jfkweatherdata-donotdelete-pr-...',\n",
    " 'asset_id': '...'}\n",
    "```\n",
    "\n",
    "**Note**: In order for this step to work, your project token (see the first cell of this notebook) must have `Editor` role. By default this will overwrite any existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e3ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T05:11:40.960566Z",
     "iopub.status.busy": "2025-05-02T05:11:40.960027Z",
     "iopub.status.idle": "2025-05-02T05:11:41.604768Z",
     "shell.execute_reply": "2025-05-02T05:11:41.603842Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_data_renamed.to_csv(\n",
    "    \"data/noaa-weather-data-jfk-airport/jfk_weather_cleaned.csv\",\n",
    "    float_format='%g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1f8a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Next steps\n",
    "\n",
    "- Close this notebook.\n",
    "- Open the `Part 2 - Data Analysis` notebook to explore the cleaned dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36034130",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"authors\"></a> \n",
    "### Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559d3b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n",
    "\n",
    "Copyright Â© 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "duration": 2.316781,
   "end_time": "2025-08-06T05:29:38.539049",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/okadak/work/QuakeFlow/elyra_examples/pipelines/introduction-to-generic-pipelines/Part 1 - Data Cleaning.ipynb",
   "output_path": "/home/okadak/work/QuakeFlow/elyra_examples/pipelines/introduction-to-generic-pipelines/Part 1 - Data Cleaning.ipynb",
   "parameters": {},
   "start_time": "2025-08-06T05:29:36.222268",
   "version": "2.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
