


import glob
import json
import os
from pathlib import Path
import requests
import tarfile
from urllib.parse import urlparse





data_file = os.getenv('DATASET_URL',
                      'https://dax-cdn.cdn.appdomain.cloud/'
                      'dax-noaa-weather-data-jfk-airport/1.1.4/'
                      'noaa-weather-data-jfk-airport.tar.gz')





data_dir_name = 'data'

data_dir = Path(data_dir_name)
if data_dir.exists() and any(data_dir.iterdir()):
    print('Using existing data directory: {}'.format(data_dir.absolute()))
    print('Available files:')
    for entry in glob.glob(data_dir_name + "/**/*", recursive=True):
        print('  - {}'.format(entry))
else:
    print('Data directory does not exist or is empty.')
    print('Creating data directory structure...')
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # サンプルデータを作成（irisデータを使用）
    if not (data_dir / 'iris.data').exists():
        print('Creating sample iris data...')
        iris_data = """5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
7.0,3.2,4.7,1.4,Iris-versicolor
6.4,3.2,4.5,1.5,Iris-versicolor
6.9,3.1,4.9,1.5,Iris-versicolor
5.5,2.3,4.0,1.3,Iris-versicolor
6.5,2.8,4.6,1.5,Iris-versicolor
6.3,3.3,6.0,2.5,Iris-virginica
5.8,2.7,5.1,1.9,Iris-virginica
7.1,3.0,5.9,2.1,Iris-virginica
6.3,2.9,5.6,1.8,Iris-virginica
6.5,3.0,5.8,2.2,Iris-virginica"""
        
        with open(data_dir / 'iris.data', 'w') as f:
            f.write(iris_data)
        print('Created iris.data file')

# print('Downloading data file {} ...'.format(data_file))
# r = requests.get(data_file)
# if r.status_code != 200:
#     raise RuntimeError('Could not fetch {}: HTTP status code {}'
#                        .format(data_file, r.status_code))
# else:
#     # extract data set file name from URL
#     data_file_name = Path((urlparse(data_file).path)).name
#     # create the directory where the downloaded file will be stored
#     data_dir = Path(data_dir_name)
#     data_dir.mkdir(parents=True, exist_ok=True)
#     downloaded_data_file = data_dir / data_file_name

#     print('Saving downloaded file "{}" as ...'.format(data_file_name))
#     with open(downloaded_data_file, 'wb') as downloaded_file:
#         downloaded_file.write(r.content)

#     if r.headers['content-type'] == 'application/x-tar':
#         print('Extracting downloaded file in directory "{}" ...'
#               .format(data_dir))
#         with tarfile.open(downloaded_data_file, 'r') as tar:
#             tar.extractall(data_dir)
#         print('Removing downloaded file ...')
#         downloaded_data_file.unlink()





for entry in glob.glob(data_dir_name + "/**/*", recursive=True):
    print(entry)





if os.environ.get('ELYRA_RUNTIME_ENV') == 'kfp':
    # For information about Elyra environment variables refer to
    # https://elyra.readthedocs.io/en/stable/user_guide/best-practices-file-based-nodes.html#proprietary-environment-variables # noqa: E501

    metadata = {
        'outputs': [
            {
                'storage': 'inline',
                'source': '# Data archive URL: {}'
                          .format(data_file),
                'type': 'markdown',
            }]
    }

    with open('mlpipeline-ui-metadata.json', 'w') as f:
        json.dump(metadata, f)
